<!DOCTYPE html>
<html lang='zh-CN'>

<head>
  <meta name="generator" content="Hexo 6.3.0">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://gcore.jsdelivr.net'>
  <link rel="preconnect" href="https://gcore.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>动手学深度学习 - 02 训练一个线性回归模型 - Coder Zhu</title>

  
    <meta name="description" content="定义一个线性模型：  y &#x3D; Wx + B + \epsilon其中，$x$ 和 $y$ 分别是输入输出数据，$W$ 和 $B$ 是参数，$\epsilon$ 是随机噪声。 现在，我们使用mxnet的随机模块，生成特征$x$和标签$y$，使用反向传播来训练这个线性回归网络，而$W$和$B$就是网络中的权重。 from mxnet import autograd, nd 生成数据num_batche">
<meta property="og:type" content="article">
<meta property="og:title" content="动手学深度学习 - 02 训练一个线性回归模型">
<meta property="og:url" content="https://vincentzhu007.github.io/2022/10/07/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%2002%20%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="Coder Zhu">
<meta property="og:description" content="定义一个线性模型：  y &#x3D; Wx + B + \epsilon其中，$x$ 和 $y$ 分别是输入输出数据，$W$ 和 $B$ 是参数，$\epsilon$ 是随机噪声。 现在，我们使用mxnet的随机模块，生成特征$x$和标签$y$，使用反向传播来训练这个线性回归网络，而$W$和$B$就是网络中的权重。 from mxnet import autograd, nd 生成数据num_batche">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-10-07T14:11:35.000Z">
<meta property="article:modified_time" content="2025-04-02T18:36:37.912Z">
<meta property="article:author" content="Vincent Zhu">
<meta property="article:tag" content="Code, Software">
<meta name="twitter:card" content="summary">
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  

  

  


  
    
      <link href="https://cdn.jsdelivr.net/npm/lxgw-wenkai-webfont@1.6.0/style.css" rel="stylesheet">
    
      <link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&display=swap" rel="stylesheet">
    
  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    

  

<header class="header"><div class="logo-wrap"><a class="title" href="/"><div class="main" ff="title">Coder Zhu</div><div class="sub cap">Veni, Vidi, Vici</div></a></div>

<nav class="menu dis-select"><a class="nav-item active" href="/">博客</a><a class="nav-item" href="/wiki">项目</a><a class="nav-item" href="/about/">关于</a></nav>
</header>


<div class="widgets">
<widget class="widget-wrapper search"><div class="widget-body"><div class="search-wrapper" id="search"><form class="search-form"><input type="text" class="search-input" id="search-input" data-filter="/blog/" placeholder="文章搜索"><svg t="1670596976048" class="icon search-icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2676" width="200" height="200"><path d="M938.2 832.6L723.8 618.1c-2.5-2.5-5.3-4.4-7.9-6.4 36.2-55.6 57.3-121.8 57.3-193.1C773.3 222.8 614.6 64 418.7 64S64 222.8 64 418.6c0 195.9 158.8 354.6 354.6 354.6 71.3 0 137.5-21.2 193.2-57.4 2 2.7 3.9 5.4 6.3 7.8L832.5 938c14.6 14.6 33.7 21.9 52.8 21.9 19.1 0 38.2-7.3 52.8-21.8 29.2-29.1 29.2-76.4 0.1-105.5M418.7 661.3C284.9 661.3 176 552.4 176 418.6 176 284.9 284.9 176 418.7 176c133.8 0 242.6 108.9 242.6 242.7 0 133.7-108.9 242.6-242.6 242.6" p-id="2677"></path></svg></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div></div></widget>


<widget class="widget-wrapper toc single" id="data-toc"><div class="widget-header cap dis-select"><span class="name">动手学深度学习 - 02 训练一个线性回归模型</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE"><span class="toc-text">生成数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-text">读取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E6%B1%82%E8%A7%A3%E7%AD%96%E7%95%A5%EF%BC%9A%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DSGD"><span class="toc-text">定义优化求解策略：随机梯度下降SGD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">训练模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83"><span class="toc-text">参考</span></a></li></ol></div></div></widget>




</div>


    </aside>
    <div class='l_main'>
      

      

    



<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> <span class="sep"></span> <a class="cap breadcrumb-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">动手学深度学习</a></div><div id="post-meta">发布于&nbsp;<time datetime="2022-10-07T14:11:35.000Z">2022-10-07</time></div></div>

<article class='md-text content post'>
<h1 class="article-title"><span>动手学深度学习 - 02 训练一个线性回归模型</span></h1>
<p>定义一个线性模型：</p>
<script type="math/tex; mode=display">
y = Wx + B + \epsilon</script><p>其中，$x$ 和 $y$ 分别是输入输出数据，$W$ 和 $B$ 是参数，$\epsilon$ 是随机噪声。</p>
<p>现在，我们使用mxnet的随机模块，生成特征$x$和标签$y$，使用反向传播来训练这个线性回归网络，而$W$和$B$就是网络中的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, nd</span><br></pre></td></tr></table></figure>
<h2 id="生成数据"><a href="#生成数据" class="headerlink" title="生成数据"></a>生成数据</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_batches = <span class="number">1000</span></span><br><span class="line">num_inputs = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">true_w = nd.array([[<span class="number">4.2</span>, <span class="number">2.0</span>, -<span class="number">3.2</span>]]).T</span><br><span class="line">true_b = nd.array([[<span class="number">2.0</span>]])</span><br><span class="line">x = nd.random.normal(<span class="number">0</span>, <span class="number">1</span>, shape=(num_batches, num_inputs))</span><br><span class="line">epsilon = nd.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, shape=(num_batches, <span class="number">1</span>)) <span class="comment"># 噪声服从均值为0,方差为0.1的正态分布</span></span><br><span class="line">y0 = nd.dot(x, true_w) + true_b + epsilon</span><br><span class="line">y = y0 + epsilon</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(true_w.shape)</span><br><span class="line"><span class="built_in">print</span>(true_b.shape)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br><span class="line"><span class="built_in">print</span>(epsilon.shape)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line"><span class="built_in">print</span>(y[<span class="number">0</span>:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<pre><code>(3, 1)
(1, 1)
(1000, 3)
(1000, 1)
(1000, 1)

[[ 1.1630787   0.4838046   0.29956347]
 [ 0.15302546 -1.1688148   1.5580711 ]
 [-0.5459446  -2.3556297   0.5414402 ]
 [ 2.6785066   1.2546344  -0.54877394]
 [-0.68106437 -0.13531584  0.37723127]
 [ 0.41016445  0.5712682  -2.7579627 ]
 [ 1.07628    -0.6141326   1.8307649 ]
 [-1.1468065   0.05383795 -2.5074806 ]
 [-0.59164983  0.8586049  -0.22794184]
 [ 0.20131476  0.35005474  0.5360521 ]]
&lt;NDArray 10x3 @cpu(0)&gt;

[[ 6.9298854]
 [-4.676832 ]
 [-6.7715883]
 [17.516018 ]
 [-2.3353257]
 [13.697228 ]
 [-0.5571796]
 [ 5.297142 ]
 [ 1.9715714]
 [ 1.8121778]]
&lt;NDArray 10x1 @cpu(0)&gt;
</code></pre><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><p>编写一个自定义的迭代器，来一批一批的获取数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">iter_batch_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter</span>():</span><br><span class="line">    <span class="comment"># 产生一个随机索引序列</span></span><br><span class="line">    idx = <span class="built_in">list</span>(<span class="built_in">range</span>(num_batches)) <span class="comment"># 生成索引序列：0,1,2,...,999</span></span><br><span class="line">    random.shuffle(idx) <span class="comment"># 随机打乱索引</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每次随机取10个数据</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_batches, iter_batch_size): <span class="comment"># 在不打乱的情况下，会生成：0，10, 20, 30 ..., 990</span></span><br><span class="line">        j = nd.array(idx[i : <span class="built_in">min</span>(i + iter_batch_size, num_batches)]) <span class="comment"># 这里设置取值范围：[0:10], [10:20],...</span></span><br><span class="line">        <span class="keyword">yield</span> nd.take(x, j), nd.take(y, j) <span class="comment"># 迭代器获取数据</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试迭代器，拿2次</span></span><br><span class="line">n = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter():</span><br><span class="line">    <span class="built_in">print</span>(data, label)</span><br><span class="line">    n += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> (n &gt;= <span class="number">2</span>):</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>[[ 4.2505121e+00 -2.1137807e-03 -7.9776019e-01]
 [ 4.9009416e-01 -1.1063820e+00  3.5573665e-02]
 [-1.2949859e-01 -3.0296946e-02 -1.7266597e-01]
 [-1.5107599e+00 -9.6534061e-01  5.4608238e-01]
 [ 7.8113359e-01 -1.1420447e+00 -2.8238511e-01]
 [ 2.6581082e-01  5.7875359e-01 -9.6763712e-01]
 [ 5.7725173e-01  3.8847062e-01 -1.2530572e+00]
 [ 1.1652315e+00  1.6189508e-01 -1.6221091e-01]
 [-2.1707486e-01 -6.4814579e-01  9.0141118e-01]
 [ 1.1282748e+00  2.5994456e+00 -2.0640564e+00]]
&lt;NDArray 10x3 @cpu(0)&gt; 
[[22.389832 ]
 [ 1.707745 ]
 [ 1.9283633]
 [-8.028843 ]
 [ 3.9277847]
 [ 7.373327 ]
 [ 9.212286 ]
 [ 7.764362 ]
 [-3.1022215]
 [18.526558 ]]
&lt;NDArray 10x1 @cpu(0)&gt;

[[ 0.1787789  -0.18420085 -0.08212578]
 [-1.3121095  -0.04268014 -1.0699745 ]
 [ 0.20131476  0.35005474  0.5360521 ]
 [-1.365017    0.69103366 -0.4321104 ]
 [-0.04626409 -1.0672387  -2.0273046 ]
 [-0.6345202  -0.10353587 -1.3175181 ]
 [ 0.7618796  -1.1695448   0.7909283 ]
 [ 0.19799927 -0.10506796 -1.3348366 ]
 [ 1.6388271   0.59673244  1.1476266 ]
 [ 2.1935298  -0.5385921  -0.8611334 ]]
&lt;NDArray 10x3 @cpu(0)&gt; 
[[ 2.6571708 ]
 [-0.15536022]
 [ 1.8121778 ]
 [-0.964692  ]
 [ 6.1714783 ]
 [ 3.366683  ]
 [ 0.34661472]
 [ 6.8972564 ]
 [ 6.4096384 ]
 [12.895386  ]]
&lt;NDArray 10x1 @cpu(0)&gt;
</code></pre><h2 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = nd.random.normal(shape=(num_inputs, <span class="number">1</span>))</span><br><span class="line">b = nd.zeros((<span class="number">1</span>,))</span><br><span class="line">params = [w, b]</span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(params)</span><br></pre></td></tr></table></figure>
<pre><code>[[-1.3058136]
 [ 0.9344402]
 [ 0.5380863]]
&lt;NDArray 3x1 @cpu(0)&gt;

[0.]
&lt;NDArray 1 @cpu(0)&gt;
[
[[-1.3058136]
 [ 0.9344402]
 [ 0.5380863]]
&lt;NDArray 3x1 @cpu(0)&gt;, 
[0.]
&lt;NDArray 1 @cpu(0)&gt;]
</code></pre><p>为参数创建梯度信息，为后面的参数求导做准备。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.attach_grad()</span><br></pre></td></tr></table></figure>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> nd.dot(x, w) + b <span class="comment"># 线性模型的预测值y_hat</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试一下模型</span></span><br><span class="line">net(data)</span><br></pre></td></tr></table></figure>
<p>​<br>    [[-0.44976735]<br>     [ 1.0977497 ]<br>     [ 0.35266796]<br>     [ 2.1956747 ]<br>     [-2.0277233 ]<br>     [ 0.02287859]<br>     [-1.6621547 ]<br>     [-1.0749872 ]<br>     [-0.9648698 ]<br>     [-3.8309872 ]]<br>    <NDArray 10x1 @cpu(0)></NDArray></p>
<h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">square_loss</span>(<span class="params">yhat, y</span>):</span><br><span class="line">    <span class="comment"># 将y reshape，避免自动广播</span></span><br><span class="line">    <span class="keyword">return</span> (yhat - y.reshape(yhat.shape)) ** <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="定义优化求解策略：随机梯度下降SGD"><a href="#定义优化求解策略：随机梯度下降SGD" class="headerlink" title="定义优化求解策略：随机梯度下降SGD"></a>定义优化求解策略：随机梯度下降SGD</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd</span>(<span class="params">params, learning_rate</span>):</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param[:] = param - learning_rate * param.grad</span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="number">5</span></span><br><span class="line">learning_rate = <span class="number">.001</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> data_iter():</span><br><span class="line">        <span class="comment"># 一次迭代学习过程</span></span><br><span class="line">        <span class="keyword">with</span> autograd.record():</span><br><span class="line">            output = net(data)</span><br><span class="line">            loss = square_loss(output, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        sgd(params, learning_rate)</span><br><span class="line">        </span><br><span class="line">        total_loss += nd.<span class="built_in">sum</span>(loss).asscalar()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch %d, average loss: %f&quot;</span> % (e, total_loss/num_batches))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 0, average loss: 12.393637
Epoch 1, average loss: 0.164320
Epoch 2, average loss: 0.002606
Epoch 3, average loss: 0.000418
Epoch 4, average loss: 0.000386
</code></pre><p>训练完，比较学到的参数和真实参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_b, b</span><br></pre></td></tr></table></figure>
<pre><code>(
 [[2.]]
 &lt;NDArray 1x1 @cpu(0)&gt;,

 [2.0011895]
 &lt;NDArray 1 @cpu(0)&gt;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">true_w, w</span><br></pre></td></tr></table></figure>
<pre><code>(
 [[ 4.2]
  [ 2. ]
  [-3.2]]
 &lt;NDArray 3x1 @cpu(0)&gt;,

 [[ 4.1996527]
  [ 2.000402 ]
  [-3.199496 ]]
 &lt;NDArray 3x1 @cpu(0)&gt;)
</code></pre><p>可以看到训练得到的参数，与真实参数非常接近。并且，随着训练次数的增加，损失函数值逐步收敛。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>B站李沐课程<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1tx41147Bp?t=3167.7">[MXNet/Gluon] 动手学深度学习第一课：从上手到多类分类</a></p>


<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2022/10/23/roadmap/">软件开发成长路线图</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2022/10/06/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%20-%2001%20%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">动手学深度学习 - 01 开发环境搭建</a></div></section></div>








      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>本站由 <a href="/">@anonymity</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> 主题创建。<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  const stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.version = '1.18.5';
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.18.5';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://gcore.jsdelivr.net/npm/jquery@3.6.2/dist/jquery.min.js'
  };

  if ('local_search') {
    stellar.search = {};
    stellar.search.service = 'local_search';
    if (stellar.search.service == 'local_search') {
      let service_obj = Object.assign({}, {"field":"all","path":"/search.json","content":true,"codeblock":true,"sort":"-date"});
      stellar.search[stellar.search.service] = service_obj;
    }
  }

  // stellar js
  stellar.plugins.stellar = Object.assign({"sites":"/js/plugins/sites.js","friends":"/js/plugins/friends.js","ghinfo":"/js/plugins/ghinfo.js","timeline":"/js/plugins/timeline.js","linkcard":"/js/plugins/linkcard.js","fcircle":"/js/plugins/fcircle.js","weibo":"/js/plugins/weibo.js"});

  stellar.plugins.marked = Object.assign("https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js");
  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/vanilla-lazyload@17.8.3/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.css","js":"https://unpkg.com/swiper@8.4.5/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://gcore.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://gcore.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://gcore.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://gcore.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti@0.9.2/umd/heti.min.css","js":"https://unpkg.com/heti@0.9.2/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
