[{"title":"旅行-2025-北京","path":"/2025/04/02/travel-2025-beijing/","content":"2025，清明，北京 - 故宫 - 天坛。 来一场心灵的栖旅，和我的宝贝。","tags":["旅行","北京"],"categories":["旅行"]},{"title":"开发工具-004-搭建jupyter远程服务","path":"/2024/09/25/devtool-001-搭建jupyter远程服务/","content":"1、安装jupyterconda create -n jupyter python=3.10pip install jupyter 2、配置jupyter设置密码，生成秘钥文件： jupyter notebook passwordEnter password:Verify password:[NotebookPasswordApp] Wrote hashed password to /your_home/.jupyter/jupyter_notebook_config.json 查看秘钥： $ cat ~/.jupyter/jupyter_notebook_config.json&#123; &quot;NotebookApp&quot;: &#123; &quot;nbserver_extensions&quot;: &#123; &quot;jupyter_nbextensions_configurator&quot;: true &#125;, &quot;password&quot;: &quot;argon2:$argon2id$v=19$m=10240,t=10,p=8$ADrvxc7AsirraAJFXTnNsg$I7KwbhJ+FXGRBW/lv3dfYw&quot; &#125;&#125; 生成默认的Jupyter配置文件，用于启动服务。 jupyter notebook --generate-config 修复配置文件选项： # 文件：~/.jupyter/jupyter_notebook_config.py# 改成False，不允许从UI修改登录密码c.NotebookApp.allow_password_change = False # 改成你自己~/.jupyter/jupyter_notebook_config.json中的password内容c.NotebookApp.password = &#x27;argon2:$argon2id$v=19$m=10240,t=10,p=8$ADrvxc7AsirraAJFXTnNsg$I7KwbhJ+FXGRBW/lv3dfYw&#x27;# 改成你自己服务器的IP地址！c.KernelManager.ip = &#x27;192.168.3.10&#x27;# 改成你需要的端口号，默认是8888c.NotebookApp.port = 8888# 设置为True，支持远程访问c.NotebookApp.allow_remote_access = True 3、安装插件pip install jupyter_contrib_nbextensionsjupyter contrib nbextensions install --userpip install jupyter_nbextensions_configuratorjupyter nbextensions_configurator enable --user 4、部署成systemd服务在miniconda目录新建一个脚本，封装jupyter启动命令： $ cat run_jupyter.sh#!/bin/bashrootDir=$(cd $(dirname $0); pwd)source $&#123;rootDir&#125;/bin/activate jupyterjupyter notebook /path/to/your_notebook_dir/ 新建systemd服务配置文件 # 文件：~/.config/systemd/user/jupyter.service [Unit]Description=jupyter daemon[Service]ExecStart=/usr/bin/bash /home/&lt;user&gt;/miniconda3/run_jupyter.sh[Install]WantedBy=default.target 启动systemd # 使能服务systemctl enable --user jupyter# 启动服务systemctl start --user jupyter# 查看服务状态systemctl status --user jupyter 参考[1] 通过 conda 安装 jupyterlab 并配置为 systemd 服务","tags":["DevTool, Jupyter"],"categories":["开发工具"]},{"title":"LLM推理02-HuggingFace LLM导出ONNX","path":"/2024/01/24/llm-infer-02-huggingface-export-onnx/","content":"官方指导：https://huggingface.co/docs/transformers/v4.35.1/zh/serialization 1、下载TinyLLM模型代码 mkdir llmkitgit clone https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0 2、编写export_onnx.py脚本，导出onnx模型 from optimum.onnxruntime import ORTModelForCausalLMfrom transformers import AutoTokenizermodel_checkpoint = &quot;./TinyLlama-1.1B-Chat-v1.0&quot;save_directory = &quot;tinyllama_onnx&quot;# 从 transformers 加载模型并将其导出为 ONNXort_model = ORTModelForCausalLM.from_pretrained(model_checkpoint, export=True)tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)# 保存 onnx 模型以及分词器ort_model.save_pretrained(save_directory)tokenizer.save_pretrained(save_directory) 结果： llmkit tree -h tinyllama_onnx[4.0K] tinyllama_onnx├── [ 697] config.json├── [2.0M] model.onnx├── [4.1G] model.onnx_data├── [ 551] special_tokens_map.json├── [1.3K] tokenizer_config.json├── [1.8M] tokenizer.json└── [488K] tokenizer.model0 directories, 7 files 输出的onnx模型大小有4G，有点奇怪，晚点再确认下原因，太晚了，睡觉。","tags":["LLM","推理","onnx","huggingface"],"categories":["LLM","推理"]},{"title":"LLM推理01-lookahead decoding性能测试","path":"/2024/01/21/llm-infer-01-lookahead-decoding-test/","content":"术语： LADE：lookahead decoding缩写。 简介LADA方法介绍：https://lmsys.org/blog/2023-11-21-lookahead-decoding/ LADE GitHub仓库：https://github.com/hao-ai-lab/LookaheadDecoding 测试流程下载并安装： git clone https://github.com/hao-ai-lab/LookaheadDecoding.gitcd LookaheadDecodingpip install -r requirements.txtpip install -e . 执行官方demo： # 不使用LADEpython minimal.py# 使用LADEUSE_LADE=1 LOAD_LADE=1 python minimal.py 提示：如果遇到报错：We couldn&#39;t connect to &#39;[https://huggingface.co](https://huggingface.co/)&#39; to load this file，须检查代理问题，确认代理可访问huggingface。 测试结果1、本地RTX-1050Ti with 4G RAM 不使用LADE：14.5 tokens/s 使用LADE：4.2 tokens/s 2、Nvidia P100 with 16G RAM 不使用LADE：40.7 tokens/s 使用LADE：26.2 tokens/s 3、Nvidia T4x2 with 16GB RAM 不使用LADE：37.2 tokens/s 使用LADE：47.0 tokens/s 性能数据汇总： 不使用LADE推理速度(tokens/s) 使用LADE推理速度(tokens/s) 性能提升比例(lade/origin - 1.0) RTX 1050Ti 14.5 4.2 -0.71 P100 40.7 26.2 -0.36 T4 x 2 37.2 47.0 0.26 分析与小结从官方Tiny-llama示例的实测结果看：（1）LADE方法在三种硬件上，只有T4x2上有26%的提示，其它两种均有显著劣化。（2）在RTX 1050Ti和P100上，输出结果的最后一行生成文本，使用LADE和不使用保持一致，但T4x2上，输出结果最后一行不一致。 官方给的数据基于llama2-7b-chat，考虑进一步用相同模型测试，对比官方数据结论。不过有意思的是：官方给的默认示例，居然是存在性能劣化的，是不是说明这种这种方法的普适性有限。","tags":["LLM","推理","lookahead"],"categories":["LLM","推理"]},{"title":"重构01、组合方法：代码重构的基础动作","path":"/2024/01/21/refactor-01-composite-method/","content":"软件设计和重构是从软件结构上对代码的简化，其中组合方法是我自己认为最为基础和关键的方法，没有之一！ 1、什么是组合方法？ 组合方法是对将函数实现为处于同一抽象层次的多个小函数组合调用的形式。简单来说，就是将长且复杂的大函数替换成多个小函数组合调用。 这里有一个关键词：同一抽象层次。举个例子来理解一下它，比如：和朋友一起出去玩，一般会讨论：去哪个景点？什么时间到？出行方式是什么？总共有几个人？总的来看，这几个点都属于“出行计划”这个场景（scenario）。写成函数： void SetTravelPlan() &#123;\tSetDestination()\tSetArrivalTime() SetTransportation() SetNumOfPeople()&#125; 在游玩计划这个特定的场景下，这些要素就属于同一抽象层次。 反过来，我们不需要讨论的是：大家各自几点起床，到达时间是否精确到具体的分钟，打车是用高德还是滴滴… 这些细节在我们出行过程中会不是涉及？会。但是影不影响出去玩？完全不影响！这些生活中习以为常的细节，其实已经包含了我们大脑的工作方式——分离关注点：把同一层次的问题放到一起，不同层次的分离开，简化要解决的问题。 2、一个应用组合方法的例子以常见的Array容器Add()方法为例，来看怎么用组合方法重构C++代码： 原始的代码（来源说明：《重构与模式》7.1示例，原例是Java实现）： template &lt;typename T&gt;class Array &#123;public:\tvoid Add(const T &amp;element) &#123; if (!readOnly_) &#123; int newSize = size_ + 1; if (newSize &gt; elements_-&gt;size()) &#123; auto newElements = std::make_unque&lt;vector&lt;T&gt;&gt;(newSize + 10); for (int i = 0; i &lt; size_; i++) &#123; newElements[i] = elements_[i]; &#125; elements_ = newElements; &#125; elements_[size_++] = element; &#125;\t&#125;private: bool readOnly_; int size_; std::unique&lt;vector&lt;T&gt;&gt; elements_;&#125; 从Add()实现看，包含了elements扩容，检查只读等所有的细节，阅读起来有压力。接下来，我们使用接近自然语言的表达，来重新概括这个函数的内容： template &lt;typename T&gt;class Array &#123;public:\tvoid Add(const T &amp;element) &#123; if (readOnly_) &#123; // (1)使用卫式语句改造 return; &#125; if (AtCapacity()) &#123; // (2) 将超出容量检查提取函数 Grow(); // (3)将扩容实现细节提取函数 &#125; AddElement(element); // (4)将添加元素实现细节提取函数\t&#125;private:\tbool AtCapacity() &#123; return size_ + 1 &gt;= elements_-&gt;size();\t&#125;\tconstexpr int kGrowSize = 10;\tvoid Grow() &#123; auto newElements = std::make_unque&lt;vector&lt;T&gt;&gt;(newSize + kGrowSize); for (int i = 0; i &lt; size_; i++) &#123; newElements[i] = elements_[i]; &#125; elements_ = newElements;\t&#125;\tvoid AddElement(const T &amp;element) &#123; elements_[size_++] = element;\t&#125;private: bool readOnly_; int size_; std::unique&lt;vector&lt;T&gt;&gt; elements_;&#125; 通过AtCapacity()、Grow()、AddElement() 三个子函数的组合调用，可以让Add()的实现更清晰简洁，这就是组合方法的应用。 3、方法分析在分析设计模式和重构时，水球潘提供了一种概念：Force，我将它浅显地解读为：从函数的A实现到B实现进行变更的因素。对于“组合方法”来说，驱动以上例子中改造的Force是：大量的细节掩盖了作者核心要表达的功能单元。只需花10s就能理解重构后Add()的内容，但是可能需要5min才能弄明白原始代码的逻辑。 参考[1] 《重构与模式》第7.1节 [2] 水球潘：Force","tags":["重构","组合","composite"],"categories":["代码重构"]},{"title":"2023-个人总结","path":"/2024/01/13/2023-个人总结/","content":"2023已经结束，是时候对全年的经历做个总结了。 回看2023，基本是被工作占满的状态，形势不好，写代码也感受到了压力。 1、回顾先来回顾一下2023年初写的年度计划。 1、写30篇博客：实际发布18篇，完成率：60%；2、掌握一门语言：Python：这个在工作中有用到，勉强入门，完成率：50%；3、深度开发一个开源项目：JetConf：这个黄了，完成率：0%；4、熟练使用设计模式：写了总共6篇博客，6/23=26%，而且实战用的很少，实际完成率：20%；5、出（远）门旅游一次：没做到，上班失去了出门的动力，完成率：0%；6、学驾驶证，没报名，完成率：0%。 总的来说，应该算是不及格，没有达到年初的计划目标。考虑到自己2023紧张的工作节奏和不太好的身体状态，这也是合理的结果。 2、反思2023，总的下来可以总结为两个字：焦虑。 焦虑在新的工作方向怎么输出，焦虑会不会失业，焦虑工作强度大会不会让身体变得更差，焦虑自己的技术竞争力在哪里，以及焦虑要不要抓紧买房，落地生根。 这些焦虑导致了畏惧、短视、心神不定。 导致这些焦虑的根因是什么？是害怕失去：害怕失去工作，害怕失去在城市生活的经济基础。 3、展望对于2024，希望自己能以平和的心态面对工作和生活，降低焦虑，更加自信，更加接纳自己。 对于工作，有个期望：希望能写一本技术书籍，让自己思考的更深刻。","tags":["计划"],"categories":["个人规划"]},{"title":"随想003. 2023，和自己和解","path":"/2024/01/08/mind-003-2023和自己和解/","content":"2023，过得很焦虑。2024，过好日子。 加油 :)","categories":["随想"]},{"title":"CUDA优化001. TensorRT加速SD的优化","path":"/2023/10/29/cuda-optimize-001-trt-on-sd/","content":"给看到的TensorRT加速AI推理2023 Hackson视频做个笔记Video。 SD模型结构： Unet ControlNet VAE Schedule latent（未入图） 优化方法： 步骤 优化操作 时延效果 分析 1 Export unet and controlnet to trt engine 2600ms→634ms 使用GPU执行 2 Use batch size 2 634ms→479ms 多batch增加计算密度 3 Remove duplicate compute in graph 479ms→472ms 消除冗余计算 4 Put the scheduler update logic in trt 5 Fuse controlnet, unet and scheduler logic to 1 engine 472ms→450ms trt8.6以后可以自动找出模型并行的部分，使用mutilstream并行执行 6 Cuda graph 450ms→425ms 单个CPU操作可发射多个GPU操作，减少KernelLaunch 7 Use plugins 425ms→404ms MHA融合算子等 8 Multi stream 404ms→393ms 将单个engine划分成3个可并行的engine，在第5步自动并行基础上提高并行度 9 Use trt 9.1 393→390ms MHA在9.1有更快实现","tags":["CUDA","GPU","性能优化"],"categories":["CUDA性能优化"]},{"title":"CUDA编程笔记004. 测量kernel耗时","path":"/2023/10/06/cuda-004-测量kernel耗时/","content":"1、使用cpu时钟测量kernel耗时#include &lt;sys/time.h&gt;double CpuSeconds() &#123; struct timeval tp; gettimeofday(&amp;tp, NULL); // 获取当前cpu时间戳。 return ((double)tp.tv_sec + (double)tp.tv_usec * 1e-6);&#125; 参考： man gettimeofday 2、检查cuda接口返回值cuda的api会统一返回cudaError_t错误码，可以通过cudaGetErrorString()获取错误码对应的错误字符串。 将校验错误码的路径归纳为下面的宏，方便使用。 #define CHECK(call) \\ do &#123; \\ const cudaError_t error = call; \\ if (error != cudaSuccess) &#123; \\ printf(&quot;Error: %s: %d, &quot;, __FILE__, __LINE__); \\ printf(&quot;code: %d, reason: %s &quot;, error, cudaGetErrorString(error)); \\ exit(1); \\ &#125; \\ &#125; while(0) 参考： API手册：cudaMalloc API手册：cudaError API手册：cudaGetErrorString 3、cuda kernel实现__global__ void kernelAdd(float *d_a, float *d_b, float *d_c, int n_elem) &#123; int id = threadIdx.x + blockIdx.x * blockDim.x; // block * grid尺寸可能大于n_elem。超过n_elem的部分，就不计算了。 if (id &lt; n_elem) &#123; d_c[id] = d_a[id] + d_b[id]; &#125;&#125; 4、调用cuda kernel// 定义block和grid的尺寸，这里只使用了一维。dim3 block(std::min(n_elem, max_thread_per_block));dim3 grid((n_elem + block.x - 1) / block.x);auto start_time = CpuSeconds();// kernel调用是异步的，此句执行后会立即返回到host侧，需要手动调用cuda同步函数等待所有线程执行完毕。kernelAdd&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_a, d_b, d_c, n_elem);// 这一行非常重要！不做手动同步，测出的耗时只是kernel异步执行api返回到host的耗时，// 并不是所有线程执行完的耗时。因此这里需要手动同步，等待所有线程执行结束。cudaDeviceSynchronize(); printf(&quot;cuda add elapsed: %.6lfs &quot;, CpuSeconds() - start_time); 根据实际计算量划分block和grid的示意图如下： 可以看到，虽然cuda生成了grid * block个线程，但最后一个block中的线程不一定会被全部用掉。 如何获取max_thread_per_block？ 在host侧设置的gridDims和blockDims，受具体的硬件限制，在不同的GPU的上限是不同的。cuda提供了接口获取当前GPU硬件的上限信息。 代码如下： cudaDeviceProp device_prop;int dev = 0;CHECK(cudaGetDeviceProperties(&amp;device_prop, dev));printf(&quot;Using device %d: %s, maxThreadsPerBlock: %d. &quot;, dev, device_prop.name, device_prop.maxThreadsPerBlock);CHECK(cudaSetDevice(dev)); 参考： API手册：cudaGetDeviceProperties 5、使用nvprof测量耗时nvprof加上所需执行的应用命令，即可执行profiling。本demo效果如下： $ nvprof ./cuda_vector_add_timingCUDA Demo: add two vectors.==191428== NVPROF is profiling process 191428, command: ./cuda_vector_add_timingUsing device 0: NVIDIA GeForce GTX 1050 Ti, maxThreadsPerBlock: 1024.testing n_elem: 16777216 ...cpu add elapsed: 0.069825scuda block:(1024, 1, 1), grid:(16384, 1, 1)cuda add elapsed: 0.002238scpu result : [1.735893, 0.446092, 1.168937, 1.654771, 1.396988, 1.141601, 0.817212, 1.561706, 0.467451, 1.386464, ...]cuda result: [1.735893, 0.446092, 1.168937, 1.654771, 1.396988, 1.141601, 0.817212, 1.561706, 0.467451, 1.386464, ...]cuda equals to cpu? yes.==191428== Profiling application: ./cuda_vector_add_timing==191428== Profiling result: Type Time(%) Time Calls Avg Min Max Name GPU activities: 67.72% 39.002ms 1 39.002ms 39.002ms 39.002ms [CUDA memcpy DtoH] 28.77% 16.571ms 2 8.2853ms 8.0925ms 8.4781ms [CUDA memcpy HtoD] 3.51% 2.0214ms 1 2.0214ms 2.0214ms 2.0214ms kernelAdd(float*, float*, float*, int) 计算两个16M数组的加法，CPU耗时70ms，GPU耗时2.23ms，是CPU的1/32。通过profiling结果可以看到，kernelAdd耗时2.02ms，比我们在host侧自己统计的耗时要短一点。这是因为nvprof能够严格测出device侧kernel执行耗时，而不必加上额外的host-device通信操作，更加精确。另外，host-device单次内存拷贝时间是8ms，是计算耗时的4倍。 参考：CUDA 专业提示：nvprof 是你便捷的通用 GPU 剖析器 小结以上通过CPU时间戳和nvprof两种方式测量了kernel执行耗时。 本文示例完整代码请见：cuda_vector_add_timing.cu。 参考[1] “Professional Cuda C Programming” Chapter 2.","tags":["CUDA","GPU","线程","block","grid","nvprof"],"categories":["CUDA编程笔记"]},{"title":"CUDA编程笔记003. 线程索引","path":"/2023/10/03/cuda-003-线程索引/","content":"1、线程索引CUDA提供了两层的层次线程模型： Grid： 定义：执行同一个kernel代码的所有thread集合称为一个grid。 位于同一grid中的thread共享相同的global memory。 一个grid包含多个block。 Block： 定义：一种thread集合，同一block的thread可通过block内同步和block内显存贡献来相互协作。 对应地，在kernel中可使用两层坐标来索引thread： 上层：blockIdx（在一个grid中的block索引）； 下层：threadIdx（在某一个block中的thread索引）。 通俗地来说，程序要执行某一个kernel时，在kernel内部想知道当前所处的线程，可以先通过blockIdx找到当前线程位于哪个block，然后在根据threadIdx找到block中具体的thread位置。 kernel，grid，block和thread的关系如下： 2、索引细节 Host侧 在host侧，由程序员自行指定CUDA kernel的grid和block维度。 kernel_name &lt;&lt;&lt;grid, block&gt;&gt;&gt;(argument list); CUDA提供了对应的数据类型是dim3。3维大小不必全部设置，可按需设置，未设置的维度默认为1，取各维度值时，可用.x, .y, .z三个成员变量。 比如6000x80的矩阵，想要划分为grid=10x20x30，block=1x20x4的话。写法如下： dim3 grid(10, 20, 30); // 指定grid中3维block的各维度大小dim3 block(1, 20, 4); // 指定block中3维thread的各维度大小testKernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;()；// 传给grid, block。 疑问1：grid和block对于kernel来说，是变量还是常量？是否在程序启动时可调。 Device侧 CUDA对kernel，预定义了两套变量，分别是： （1）获取维度： gridDim(uint3类型):和host侧grid值一致。 blockDim(uint3类型)：和host侧block值一致。 （2）获取各维度索引： blockIdx(uint3类型)：grid中block的索引值； threadIdx(uint3类型)：指定block中thread的索引值。 3、动手测试Demo代码（GitHub）： __global__ void CheckIndex(void) &#123; printf(&quot;Device: threadIdx:(%d, %d, %d), blockIdx:(%d, %d, %d), &quot; &quot;blockDim:(%d, %d, %d), gridDim:(%d, %d, %d) &quot;, threadIdx.x, threadIdx.y, threadIdx.z, blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z, gridDim.x, gridDim.y, gridDim.z);&#125;int main() &#123; printf(&quot;CUDA Demo: illustrate block and thread index. &quot;); int n_elem = 6; // 计算block和grid数量：1个grid包含多个block，1个block包含多个thread. dim3 block(3); // block内的thread分布 dim3 grid((n_elem + block.x - 1) / block.x); // grid中的block分布 printf(&quot;Host: block:(%d, %d, %d), grid:(%d, %d, %d) &quot;, block.x, block.y, block.z, grid.x, grid.y, grid.z); CheckIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;(); cudaDeviceReset();&#125; 打印结果： Host: block:(3, 1, 1), grid:(2, 1, 1)Device: threadIdx:(0, 0, 0), blockIdx:(0, 0, 0), blockDim:(3, 1, 1), gridDim:(2, 1, 1)Device: threadIdx:(1, 0, 0), blockIdx:(0, 0, 0), blockDim:(3, 1, 1), gridDim:(2, 1, 1)Device: threadIdx:(2, 0, 0), blockIdx:(0, 0, 0), blockDim:(3, 1, 1), gridDim:(2, 1, 1)Device: threadIdx:(0, 0, 0), blockIdx:(1, 0, 0), blockDim:(3, 1, 1), gridDim:(2, 1, 1)Device: threadIdx:(1, 0, 0), blockIdx:(1, 0, 0), blockDim:(3, 1, 1), gridDim:(2, 1, 1)Device: threadIdx:(2, 0, 0), blockIdx:(1, 0, 0), blockDim:(3, 1, 1), gridDim:(2, 1, 1) 参考[1] “Professional Cuda C Programming” Chapter 2.","tags":["CUDA","GPU","线程","block","grid"],"categories":["CUDA编程笔记"]},{"title":"CUDA编程笔记002. Hello CUDA","path":"/2023/09/30/cuda-002-Hello-CUDA/","content":"使用CMake开发第一个CUDA工程hello-cuda。 源代码目录如下： $ tree hello-cuda/.├── CMakeLists.txt└── hello_cuda.cu 1、编写CMakeList.txt cmake_minimum_required(VERSION 3.14)# CMake支持CUDA语言，会自动识别.cu文件，并使用nvcc对齐编译。project(hello-cuda CXX CUDA)# 定义可执行程序hello_cuda，对应的源文件是hello_cuda.cu。add_executable(hello_cuda hello_cuda.cu) 2、编写hello_cuda.cu #include &lt;stdio.h&gt;/* __global__表示这是一个CUDA kernel，在device侧，也就是GPU上运行。*/__global__ void helloFromGPU(void) &#123; printf(&quot;Hello World from GPU! &quot;);&#125;int main(void) &#123; printf(&quot;Hello World from CPU! &quot;); // 执行GPU kernel函数，会有5个GPU thread各打印1个。 helloFromGPU &lt;&lt;&lt;1, 5&gt;&gt;&gt;(); // 销毁当前进程所管理设备的所有资源。如果不加这行，会看不到GPU打印! cudaDeviceReset(); return 0;&#125; 3、编译cuda程序 cmake -S . -B build/cd buildmake 提示： 如果cmake报错：No CMAKE_CUDA_COMPILER could be found，可配置环境变量 CUDACXX=/usr/local/cuda-12.2/bin/nvcc，再执行cmake即可。 4、执行cuda程序 $ ./hello_cudaHello World from CPU!Hello World from GPU!Hello World from GPU!Hello World from GPU!Hello World from GPU!Hello World from GPU! 输出CPU打印1个，GPU打印5个。 5、编译细节 通过make clean; make VERBOSE=1可以看到具体的编译器命令参数： [ 50%] Building CUDA object CMakeFiles/hello_cuda.dir/hello_cuda.cu.o/usr/local/cuda-12.2/bin/nvcc -forward-unknown-to-host-compiler -MD -MT CMakeFiles/hello_cuda.dir/hello_cuda.cu.o -MF CMakeFiles/hello_cuda.dir/hello_cuda.cu.o.d -x cu -c /home/zgd/code/CppKit/src/cuda/hello_cuda.cu -o CMakeFiles/hello_cuda.dir/hello_cuda.cu.o[100%] Linking CUDA executable hello_cuda/usr/bin/cmake -E cmake_link_script CMakeFiles/hello_cuda.dir/link.txt --verbose=1/usr/bin/g++ CMakeFiles/hello_cuda.dir/hello_cuda.cu.o -o hello_cuda -lcudadevrt -lcudart_static -lrt -lpthread -ldl -L&quot;/usr/local/cuda-12.2/targets/x86_64-linux/lib/stubs&quot; -L&quot;/usr/local/cuda-12.2/targets/x86_64-linux/lib&quot; 可以看到，CMake cuda程序的编译流程： 首先，使用nvcc将hello_cuda.cu编译成hello_cuda.cu.o； 然后，使用g++将hello_cuda.cu.o和cuda库cudart_static、cudadevrt链接成可执行文件hello_cuda。 以上便是cuda的第一个入门实例。","tags":["CUDA","GPU"],"categories":["CUDA编程笔记"]},{"title":"CUDA编程笔记001. 安装CUDA","path":"/2023/09/30/cuda-001-安装CUDA/","content":"操作环境：Dell笔记本 Ubuntu 22.04 x86_64 Nvidia Geforce GTX 1050Ti 安装指导：https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html 3.10章节 1、检查显卡硬件 $ lspci | grep -i nvidia01:00.0 VGA compatible controller: NVIDIA Corporation GP107M [GeForce GTX 1050 Ti Mobile] (rev a1)01:00.1 Audio device: NVIDIA Corporation GP107GL High Definition Audio Controller (rev a1) 上述信息表示GPU驱动已安装完毕。 2、安装内核头文件和开发库 sudo apt-get install linux-headers-$(uname -r) 检查是否安装成功： pkg-query -s linux-headers-$(uname -r) 3、安装CUDA wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.debsudo dpkg -i cuda-keyring_1.1-1_all.debsudo apt-get updatesudo apt-get -y install cuda 4、设置CUDA环境变量 在~/.bashrc或~/.zshrc中追加以下命令：export PATH=/usr/local/cuda-12.2/bin$&#123;PATH:+:$&#123;PATH&#125;&#125; 5、重启机器。 sudo reboot 6、验证安装成功 查看驱动版本： cat /proc/driver/nvidia/version 查看GPU nvidia-smi 运行官方cuda示例：https://github.com/nvidia/cuda-samples $ git clone https://github.com/NVIDIA/cuda-samples.git$ cd cuda-samples$ make$ ./bin/x86_64/linux/release/asyncAPI[./bin/x86_64/linux/release/asyncAPI] - Starting...GPU Device 0: &quot;Pascal&quot; with compute capability 6.1CUDA device [NVIDIA GeForce GTX 1050 Ti]time spent executing by the GPU: 13.18time spent by CPU in CUDA calls: 6.04CPU executed 29511 iterations while waiting for GPU to finish 运行成功，表示CUDA正常工作。 以上便是CUDA的安装流程。","tags":["CUDA","GPU"],"categories":["CUDA编程笔记"]},{"title":"随想002. 要去见客户","path":"/2023/08/18/mind-002-要去见客户/","content":"突然接到通知，要去客户公司做技术支持。对于我自己来说，这是第一次“见客户”。有期待也有忐忑，毕竟自己刚刚接手这个新的业务，但必须得走出舒适区，见见世面。 我自己一直做事都偏向于：做好充足准备再动手。这次不一样，得随机应变了。看看结果如何…","categories":["随想"]},{"title":"随想001. 梅西的奇迹","path":"/2023/08/16/mind-001-梅西的奇迹/","content":"见证梅西创造了奇迹！这个夏天带领迈阿密队6连胜，从联盟倒数第一到进入半决赛，再一次创造历史。 向梅西学习，解决掉项目的屎山代码，期待下半年创造一个属于自己的软件奇迹！","categories":["随想"]},{"title":"C++杂谈003. 聊聊TDD","path":"/2023/08/16/cpptalk-003-tdd/","content":"TDD（Test-Driven Development）是一种敏捷开发方式，主张：先写测试，再实现功能。 TDD的核心流程可以总结为：红绿重构，感兴趣的话可以去看Kent Beck的《测试驱动开发》，这里仅记录几点对TDD的思考。 思考1、测试驱动开发 vs 测试先于开发 TDD真的只是“先写测试，再写开发”吗？如果只是这样的话，为啥不叫测试先于开发，或者先测试后开发？ 这里的关键是如何理解“驱动”二字。个人理解，它隐含着从过程向目标的逼近。测试用例是预期的”目标“，这个目标可大可小，简单的比如解析一个字符串参数，也可能复杂到比如：求解一个图的最短路（对我来说确实挺难的）。但用例一旦写出来，就划定了目标，告诉开发者这个功能的表现是什么样的。另一方面，开发功能代码是”过程“，不断的去接近并最终达到目标，让用例通过，这就是我自己所理解的“驱动”的内涵：以终为始，先定目标，再向目标冲刺。 第2个点，TDD中的T并不是指所有种类的测试，这里专指Unit Test。UT区别于其它类型的测试的是，它的粒度很细，测试对象是函数粒度。前面说用例是“目标”，那这么小的目标意味着什么？这就需要”任务分解“。不断地把大的、不能立即实现的功能拆分成：小的、可轻易实现的细粒度功能。不断地准备好这些”水泥沙子“，高楼大厦的目标也就不远了。 思考2、TDD意味着高质量的编码，不做低效重复功 使用TDD开发的代码，是具有自验证性质的代码。当代码有bug时，通过用例集，可以快速分析定界。另外，用例集本身构筑了一道“墙”，墙内是充分验证过的区域，可以提供承诺的功能；墙外是未定义的行为或各种灾难的场景，这不是我们的软件所能处理的。有了这道墙，才可以衡量代码的好坏，才会逐渐形成高质量的编码。 思考3、TDD不应该是强制约束，而应该是一种选择。 刚接触TDD时，会觉得很别扭：作为一个开发，编码为啥要从用例开始写？其实不必为了TDD而TDD，TDD固然好，但按照常规的方式——先写功能代码再写UT也没啥问题，核心在于：需要想清楚将要开发的特性表现的行为是什么？并且这种行为要能够被自动看护起来（为了不在相同的问题重复投入）。这样看，TDD只是恰好符合这一点的开发模式之一而已，它只是”术“，不是”道“，不必苛求，但确实可以作为一种不错的选择。","tags":["TDD","软件设计"],"categories":["C++杂谈"]},{"title":"C++杂谈002. 接口与智能指针的结合","path":"/2023/08/15/cpptalk-002-interface-and-unique-ptr/","content":"类(对象)是C++的第一公民，类的接口继承（通过基类指针调用具体类的重载方法）是实现模块解耦的常用方法，而 比如，一个经典的继承例子，Student和Teacher都继承自Person，拥有GetName()方法。 class Person &#123;public:\tvirtual std::string GetName() const = 0;&#125;class Teacher: public Person &#123;public:\tTeacher(const std::string &amp;name): name_(name) &#123;&#125;\tstd::string GetName() const override &#123; return name_; &#125;\tvoid Teach() &#123;&#125;private:\tstd::string name_;&#125;class Student: public Person &#123;public:\tStudent(const std::string &amp;name): name_(name) &#123;&#125;\tstd::string GetName() const override &#123; return name_; &#125; void Learn() &#123;&#125;private:\tstd::string name_;&#125; 如果遍历所有角色，可以直接使用基类Person指针来实现。 std::vector&lt;Person *&gt; CreateUsers() &#123;\tstd::vector&lt;Person *&gt; users;\tPerson *one_teacher = new Teacher(&quot;ZhangSan&quot;);\tusers.push_back(one_teacher);\tPerson *one_student = new Student(&quot;XiaoMing&quot;);\tusers.push_back(one_student);\tPerson *another_student = new Student(&quot;XiaoHua&quot;);\tusers.push_back(another_student); return users;&#125;void PrintUsers(const std::vector&lt;Person *&gt; &amp;users) &#123;\tfor (user: users) &#123; std::cout &lt;&lt; user-&gt;GetName() &lt;&lt; std::endl;\t&#125;&#125;int main() &#123;\tauto users = CreateUsers();\tPrintUsers(users);&#125; 上面的代码是典型的接口调用，但是裸指针的内存管理比较麻烦，容易存在内存泄漏或者double free，这里结合智能指针来改造一下。 using PersonPtr = std::unique_ptr&lt;Person&gt;;std::vector&lt;PersonPtr&gt; CreateUsers() &#123;\tstd::vector&lt;PersonPtr&gt; users; PersonPtr one_teacher = std::make_unique&lt;Teacher&gt;(&quot;ZhangSan&quot;);\tusers.push_back(one_teacher);\tPersonPtr one_student = std::make_unique&lt;Student&gt;(&quot;XiaoMing&quot;);\tusers.push_back(one_student);\tPersonPtr another_student = std::make_unique&lt;Student&gt;(&quot;XiaoHua&quot;);\tusers.push_back(another_student); return users;&#125; 这样就可以用智能指针管理接口对应的各种实体对象了。","tags":["C++","智能指针","接口"],"categories":["C++杂谈"]},{"title":"动手学深度学习 - 03 Stable Diffusion初体验","path":"/2023/06/15/动手学深度学习 - 03 Stable-Diffusion初体验/","content":"简介Stabel Diffusion，简称SD，中文翻译成稳定扩散模型，是一种输入文字生成图片的AI模型。 算法模型如下所示：（图来源参见[1]） 体验以下是基于Windows平台 + ONNXRuntime-GPU，对SD的一个初步体验记录。 1、运行平台 Windows 10 GTX 1050Ti-4G. 2、下载代码# 下载ONNX示例工程git clone https://github.com/cassiebreviu/StableDiffusion.git# 下载sd v1.5 onnx模型git clone https://huggingface.co/runwayml/stable-diffusion-v1-5 -b onnx 3、 安装依赖包： Visual Studio 2022（代码运行需要.Net 6，对应的VS是2022） Cuda（安装完毕，需更新显卡驱动到最新，否则推理时可能会报错） cuDNN（可能缺失zipdll，需下载） onnxruntime-x64-gpu 4、 运行推理 生成图片： 提示词1：”a fireplace in an old cabin in the woods”。 推理时GPU状态： 提示词2：”a dog chasing a car” 小结 只是简单换了几组提示词，生成效果比前面用的一些图片生成网站要好。 比较惊喜的是，在我性能一般，显存只有4G的游戏笔记本上，也能成功运行，而且15次推理大约13s的时延，是非常不错的。 参考[1] Stable Diffusion原理解读[2] https://onnxruntime.ai/docs/tutorials/csharp/stable-diffusion-csharp.html","tags":["GPU","Stabel Diffusion","ONNX"],"categories":["深度学习","动手学深度学习"]},{"title":"浅尝辄止DevOps 001. Jenkins使用入门","path":"/2023/04/16/devops-01-jenkins-startup/","content":"Jenkins是用Java编写的一款CI &amp; CD（Continious Integration 持续集成，Continious Delivery 持续交付）工具。简单来说，它提供的功能可以归纳为：软件流程“自动化”。 从“自动化”说起郑晔老师在《10x 程序员工作法》中，总结了关于高效程序员的四个要素： 以终为始 任务分解 沟通反馈 自动化 “自动化” 的最大作用是：帮助程序员从繁杂重复的工作中解脱出来，也就是”偷懒“。我自己平时就有攒shell脚本的习惯，把平时使用的一些工作脚本收集起来，随时修改复用或者直接使用。这些脚本，算是对人工操作的一个记录，在需要时重放，降低人的负担，同时也能利用上计算机执行过程可重复的优点。 Jenkins是什么？Jenkins解决的，是软件开发过程流程自动化问题，其中最突出的环节也就是 CI &amp; CD。 CI（持续集成） 和 CD（持续交付） 是 敏捷开发 所提倡的开发方法。我个人把它的观点总结为： 软件开发是一项复杂工程，为了降低项目失败的风险，需要尽早发现软件的问题，并快速解决。 推向极限，就是在代码提交上库过程中随时验证、尽可能缩短从代码“开发态”到“可交付态”的Gap，最终做到代码随时可交付。 CI的应用，举个例子，代码提交到仓库时，需要构建、测试，看看有没有引入Bug，导致程序异常。其实这个流程的各个步骤基本是固定的，可以很方便的用Jenkins来自动化。实际上，很多项目用的就是Jenkins：当程序员向主仓提交一个PR后，会触发预先配置好的git hook脚本（可以笼统理解为触发一个PR提交事件），它会通知Jenkins去拉取此PR，尝试合入到主仓最新代码上，然后编译、测试，这样来验证合入PR是否存在明显问题，这便是最典型的应用场景。 Jenkins是一个框架，规定了问题解决的范式，但具体的细节，需要根据项目自身情况去配置，比如：项目用什么语言开发；用什么命令构建项目；如何执行测试；如何打包交付等等。 开始体验Jenkins安装Jenkins这里采用Docker的形式来部署Jenkins，好处是方便迁移，并且通过容器隔离避免污染服务器环境。 演示系统：MacOS 13.2 / Intel CPU 前置条件：请确保已安装Docker（以MacOS为例，可去Docker官网下载dmg安装镜像） （1）获取jenkins Docker镜像 这里使用Blue Ocean版本，在国内安装插件也更容易成功。 $ docker pull jenkinsci/blueocean 效果： （2）运行Docker镜像 $ docker run \\--name jenkinsci-blueocean -u root \\--rm -d -p 7005:8080 -p 50000:50000 \\-v /Users/Xyz/SourceCode/docker/data:/var/jenkins_home \\ # 请替换成你自己环境中的路径!!!-v /var/run/docker.sock:/var/run/docker.sock jenkinsci/blueocean 参数解读： -d：Detached，指定容器在后台运行； —rm：如果已经存在运行的容器，就删除它； -v 宿主机路径:容器内路径：挂载卷，这里将宿主端的/Users/Xyz/SourceCode/docker/data挂载到容器内的/var/jenkins_home，另外，将宿主端的 /var/run/docker.sock 文件挂载到容器的 /var/run/docker.sock 保证容器内的docker与服务器上docker的通讯； -p 宿主机端口:容器内端口：，将容器内的端口映射到host侧；Jenkins代理默认通过TCP端口50000与Jenkins主机通信。 -u root：指定用户为root，防止出现执行权限问题； 启动完毕后，在浏览器输入：127.0.0.1:7005 访问Jenkins网页（首次启动需要等待jenkins初始化环境），可用Docker查看容器日志： 下一步，网页会提示：解锁须输入管理员密码，在上方截图的容器日志中找到。输入密码重启Jenkins容器，就可以开始使用了。 配置Jenkins首先是插件安装，Jenkins提供了大量扩展插件，方便用户使用，这里选择 “安装推荐插件”。（如果有少数插件安装报错，可以跳过） 输入 http://localhost:7005/，访问主页。 下面以我自己的一个开源C++项目：vincentzhu007/flatbuffers_demo为例，介绍基本配置流程。 安装C++编译工具链 Jenkins Docker镜像采用的OS是Alpine Linux，从Docker控制台登入Jenkins容器环境，安装C++工具链： $ apk update$ apk add cmake make gcc g++ 配置Jenkins项目 （1）输入名称：flatbuffers_demo_ci，选择“Multi-configuration project”类型，点击OK。 （2）在“Build Environment”栏勾选“Delete workspace before build starts”，这样可以让每次构建都是基于干净的空目录。 （3）在“Build”栏，点击“Add build step”，选择“Execute shell”，输入以下脚本： echo &quot;Start to download repo...&quot;git clone https://github.com/vincentzhu007/flatbuffers_demo.gitecho &quot;Start to build...&quot;cd flatbuffers_demomkdir build &amp;&amp; cd buildcmake .. &amp;&amp; make -jecho &quot;Finished build.&quot; 点击“Save”保存，会跳转到“Project flatbuffer_demo_ci”页面。 （4）在左边栏点击“Build Now”，手动触发任务。在左下角便有编号为“#1”的流水线任务生成，点击跳转，可以查看流水线详情，在左边栏，点击“Console Output”可以查看构建日志。 以上是最简单的“手动触发流水线 + 内置构建节点”使用方式，后续会进一步介绍：流水线触发事件、配置分布式构建节点等内容。 参考[1] Jenkins官方教程：使用Blue Ocean生成Pipeline[2] 知乎：手把手教你使用 Jenkins+Docker 实现持续集成","tags":["DevOps, Jenkins"],"categories":["浅尝辄止DevOps"]},{"title":"开发工具-002-hexo不显示图片","path":"/2023/02/23/tools-001-hexo-not-show-image/","content":"问题hexo的markdown文章渲染后不能显示本地图片。 解法1、配置hexo根目录__config.yml： post_asset_folder: true 2、在hexo博客根目录安装`` npm install https://github.com/CodeFalling/hexo-asset-image --save 3、在博客的source/_posts下创建和md文件同名的目录，放入图片，然后在md中输入： ![](目录名/文件名.png) 参考[1] hexo中图片无法加载","tags":["hexo","图片"],"categories":["开发工具"]},{"title":"AI杂谈001. 使用TFLite实现MNIST推理","path":"/2023/02/22/aitalk-001-mnist-with-tflite/","content":"import tensorflow as tffrom tensorflow import kerasimport numpy as npimport matplotlib.pyplot as pltimport randomprint(tf.__version__) 2023-02-22 22:47:18.176362: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. 2.11.0 下载MNIST数据集mnist = keras.datasets.mnist(train_images, train_labels), (test_images, test_labels) = mnist.load_data() Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11490434/11490434 [==============================] - 6s 0us/step # 归一化图片像素值至0-1间train_images = train_images / 255.0test_images = test_images / 255.0 # 绘制训练集的前25张图片plt.figure(figsize=(10,10))for i in range(25): plt.subplot(5, 5, i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) plt.imshow(train_images[i], cmap=plt.cm.gray) plt.xlabel(train_labels[i])plt.show() ​​ 定义模型# 定义模型结构model = keras.Sequential([ keras.layers.InputLayer(input_shape=(28, 28)), keras.layers.Reshape(target_shape=(28, 28, 1)), keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation=tf.nn.relu), keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation=tf.nn.relu), keras.layers.MaxPooling2D(pool_size=(2, 2)), keras.layers.Dropout(0.25), keras.layers.Flatten(), keras.layers.Dense(10)])# 定义如何训练模型model.compile(optimizer=&#x27;adam&#x27;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#x27;accuracy&#x27;])#训练模型model.fit(train_images, train_labels, epochs=5) 1875/1875 [==============================] - 44s 23ms/step - loss: 0.1370 - accuracy: 0.9589 Epoch 2/5 1875/1875 [==============================] - 47s 25ms/step - loss: 0.0534 - accuracy: 0.9835 Epoch 3/5 1875/1875 [==============================] - 48s 26ms/step - loss: 0.0401 - accuracy: 0.9876 Epoch 4/5 1875/1875 [==============================] - 49s 26ms/step - loss: 0.0312 - accuracy: 0.9897 Epoch 5/5 1875/1875 [==============================] - 50s 27ms/step - loss: 0.0256 - accuracy: 0.9919 model.summary() Model: &quot;sequential&quot; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= reshape_1 (Reshape) (None, 28, 28, 1) 0 conv2d_2 (Conv2D) (None, 26, 26, 32) 320 conv2d_3 (Conv2D) (None, 24, 24, 64) 18496 max_pooling2d_1 (MaxPooling (None, 12, 12, 64) 0 2D) dropout_1 (Dropout) (None, 12, 12, 64) 0 flatten_1 (Flatten) (None, 9216) 0 dense_1 (Dense) (None, 10) 92170 ================================================================= Total params: 110,986 Trainable params: 110,986 Non-trainable params: 0 _________________________________________________________________ 评估模型test_loss, test_acc = model.evaluate(test_images, test_labels)print(&quot;Test accuracy:&quot;, test_acc) 313/313 [==============================] - 2s 7ms/step - loss: 0.0310 - accuracy: 0.9903 Test accuracy: 0.9902999997138977 def get_label_color(val1, val2): if (val1 == val2): return &#x27;green&#x27; else: return &#x27;red&#x27; predictions = model.predict(test_images)# 模型输出10个浮点数，表示输入图片中是0到9的概率，需要找出最大概率的值，也就是预测的最可能的数字prediction_digits = np.argmax(predictions, axis=1)plt.figure(figsize=(18, 18))for i in range(100): ax = plt.subplot(10, 10, i+1) plt.xticks([]) plt.yticks([]) plt.grid(False) image_index = random.randint(0, len(prediction_digits)) plt.imshow(test_images[image_index], cmap=plt.cm.gray) ax.xaxis.label.set_color(get_label_color(prediction_digits[image_index], test_labels[image_index])) plt.xlabel(&#x27;Predicted: %d&#x27; % prediction_digits[image_index])plt.show() 313/313 [==============================] - 2s 7ms/step 转换为tflite模型# 将Keras模型转换为TF Lite浮点模型converter = tf.lite.TFLiteConverter.from_keras_model(model)tflite_float_model = converter.convert()# 显示浮点模型大小float_model_size = len(tflite_float_model) / 1024print(&#x27;Float model size = %dKBs&#x27; % float_model_size) WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmp614pyr8m/assets INFO:tensorflow:Assets written to: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmp614pyr8m/assets Float model size = 437KBs 2023-02-22 23:22:07.283679: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format. 2023-02-22 23:22:07.283696: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency. 2023-02-22 23:22:07.284211: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmp614pyr8m 2023-02-22 23:22:07.285685: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags &#123; serve &#125; 2023-02-22 23:22:07.285696: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmp614pyr8m 2023-02-22 23:22:07.289811: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled 2023-02-22 23:22:07.290950: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle. 2023-02-22 23:22:07.323036: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmp614pyr8m 2023-02-22 23:22:07.331699: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags &#123; serve &#125;; Status: success: OK. Took 47489 microseconds. 2023-02-22 23:22:07.351673: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable. # 使用量化方式重新转换模型converter.optimizations = [tf.lite.Optimize.DEFAULT]tflite_quantized_model = converter.convert()# 显示量化模型大小quantized_model_size = len(tflite_quantized_model) / 1024print(&#x27;Quantized model size = %dKBs&#x27; % quantized_model_size) WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading. INFO:tensorflow:Assets written to: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmpio1p9wql/assets INFO:tensorflow:Assets written to: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmpio1p9wql/assets Quantized model size = 114KBs 2023-02-22 23:24:13.132530: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format. 2023-02-22 23:24:13.132544: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency. 2023-02-22 23:24:13.132644: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmpio1p9wql 2023-02-22 23:24:13.134124: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags &#123; serve &#125; 2023-02-22 23:24:13.134133: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmpio1p9wql 2023-02-22 23:24:13.138687: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle. 2023-02-22 23:24:13.172666: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/mp/n8hmdv6j5y1f9th2v0k78zz80000gp/T/tmpio1p9wql 2023-02-22 23:24:13.181701: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags &#123; serve &#125;; Status: success: OK. Took 49057 microseconds. 运行tflite模型def evaluate_tflite_model(tflite_model): # 初始化tflite解释器 interpreter = tf.lite.Interpreter(model_content=tflite_model) interpreter.allocate_tensors() input_tensor_index = interpreter.get_input_details()[0][&quot;index&quot;] output = interpreter.tensor(interpreter.get_output_details()[0][&quot;index&quot;]) # 对测试集的每张图片运行推理 prediction_digits = [] for test_image in test_images: # 预处理：添加batch维，并且转换成float32类型，来匹配模型的输入格式 test_image = np.expand_dims(test_image, axis=0).astype(np.float32) interpreter.set_tensor(input_tensor_index, test_image) # 运行推理 interpreter.invoke() # 后处理：消除batch维，并且找出可能性最高的数字 digit = np.argmax(output()[0]) prediction_digits.append(digit) # 比较预测结果和标杆标签，计算精度 accurate_count = 0 for index in range(len(prediction_digits)): if prediction_digits[index] == test_labels[index]: accurate_count += 1 accuracy = accurate_count * 1.0 / len(prediction_digits) return accuracyfloat_accuracy = evaluate_tflite_model(tflite_float_model)print(&#x27;Float model accuracy = %.4f&#x27; % float_accuracy)quantized_accuracy = evaluate_tflite_model(tflite_quantized_model)print(&#x27;Quantized model accuracy = %.4f&#x27; % quantized_accuracy) Float model accuracy = 0.9903 Quantized model accuracy = 0.9903 参考[1] TensorFlow示例","tags":["MNIST","TFLite","AI"],"categories":["AI杂谈"]},{"title":"C++杂谈001. exit()时的对象析构","path":"/2023/02/21/cpptalk-001-dtor-with-exit/","content":"当调用exit()函数终止进程时，会触发C++静态和全局对象的析构，但局部对象不会被析构。 一个简单的demo程序如下： class Foo &#123; public: Foo(const std::string &amp;desc) : desc_(desc) &#123; cout &lt;&lt; &quot;call Foo ctor of &quot; &lt;&lt; desc_ &lt;&lt; endl; &#125; ~Foo() &#123; cout &lt;&lt; &quot;call Foo dtor of &quot; &lt;&lt; desc_ &lt;&lt; endl; &#125; private: std::string desc_;&#125;;class Bar &#123; private: static Foo foo_;&#125;;Foo Bar::foo_(&quot;static in class&quot;);static Foo foo01(&quot;static&quot;);Foo foo02(&quot;global&quot;);int main() &#123; Foo foo03(&quot;local&quot;); static Foo foo04(&quot;local static&quot;); cout &lt;&lt; endl; exit(0);&#125; 执行结果如下： call Foo ctor of static in classcall Foo ctor of staticcall Foo ctor of globalcall Foo ctor of localcall Foo ctor of local staticcall Foo dtor of local staticcall Foo dtor of globalcall Foo dtor of staticcall Foo dtor of static in class 可以看到：1、无论是全局对象，还是各种静态对象（文件内静态对象、类的静态对象、函数内静态对象）都执行了析构，而main()函数中局部对象没有析构。2、全局/静态对象（不在函数中）构造的顺序是在代码中出现的先后位置，析构的顺序与构造顺序相反。 ISO C++ 2003中关于exit()的行为规范： 参考[1] 知乎问题：C++ exit时没调析构？[2] CSDN：exit函数和析构函数的关系","tags":["C++","析构","exit"],"categories":["C++杂谈"]},{"title":"开发工具-001-如何使用Hexo构建个人博客","path":"/2023/01/27/how-to-build-blog-with-hexo/","content":"本文记录Hexo入门使用流程。 1、安装Hexo Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 Hexo的运行依赖于Node.js。首先安装Node.js，然后使用Node.js中自带的包管理工具npm安装Hexo模块。 本文以MAC OS为例演示Hexo安装，其它系统稍有区别，可参考结尾文档链接。 brew install nodesudo npm install -g hexo-cli 带-g表示全局安装，须使用sudo安装，这样可以直接使用hexo命令（本地开发推荐）；不带-g时只能用npx hexo。 2、创建博客工程在本地文件夹下用hexo创建博客工程目录： hexo init myblog # myblog可以替换成你自己的博客目录名，比如：ZhangsanBlog 待下载安装完必要的npm包，并初始化完毕。进入myblog目录，执行hexo server本地渲染网页。 cd ./mybloghexo server 打开本地网站：http://localhost:4000/，即可看到hexo生成的网页内容。当博客内容被修改时，刷新网页即可更新预览。 3、新增文章在工程根目录myblog/下执行： hexo new post &#x27;测试&#x27; 会在source/_posts/下面生成一个Markdown文件：测试.md。内容如下： ---title: 测试date: 2023-01-27 16:15:38tags:--- 在上面模板的下方新增Markdown内容，便是文章的正文，刷新本地浏览器界面，即可预览效果。 4、生成静态网站手动生成静态网站代码： hexo clean &amp;&amp; hexo generate 即可在public/子目录生成对应的网页代码。将里面的内容可以打包部署到个人服务器，或者Gitgub Pages，即可搭建最基础的博客。 参考教程Hexo安装https://hexo.io/zh-cn/docs/","tags":["Hexo","博客","工具"],"categories":["开发工具"]},{"title":"2023年度计划","path":"/2023/01/27/2023-plan/","content":"春节假期告急。 在这个阳光明媚的假期下午，立几个年度目标，给自己画个方向。 写30篇博客； 大概2周一篇的样子，字数不限，内容不限，目的是为了记录和总结。 掌握一门语言：Python； 能够自己写一个Python项目的程度； 深度开发一个开源项目：JetConf； 目标是github达到5颗星； 熟练使用设计模式； 设计模式是软件设计的一个关键技能。需要了解23种模式的背景、问题，以及模式的演进变种。 目标是写一个设计模式教程，能够教会别人才是真的学会。 出门旅游一次； 3年疫情，已经可以看到终点。时间宝贵，抓紧好好享受人生。 大概的想法是：必须出市区，最好是香港、西藏。看来港澳台通行证需要准备起来了。 学驾驶证 以前读书，因为考研、找实习，总是拖着不学车，其实心底是对这件事没底。越怕的事越早做掉，减少焦虑。抓紧报名，学起来! 希望2023是充实的一年。","tags":["计划"],"categories":["个人规划"]},{"title":"Hello World","path":"/2023/01/15/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment"},{"title":"FlatBuffer Startup","path":"/2023/01/07/FlatBuffer-Startup/","content":"What’s FlatBuffers? FlatBuffers is a cross platform serialization library. Official Website：https://google.github.io/flatbuffers/ Source Code: https://github.com/google/flatbuffers How to Use it?Normally, there are five steps: (1). Define a schema: Write a schema file to define the structure. (2). Generate C++(or other languages) code: Use flatc to generate C++ code to access and construct serialized data. (3). Invoke C++ API to serialize the object(encode): Use FlatBufferBuilder class to construct a flat binary buffer. (4). Buffer Persistency: Store and send your buffer file. (5). Invoke C++ API to deserialize(decode): Read the buffer back and obtain the pointer to the root. Build It From SourceFirstly, let’s start to build it with follow steps. $ git clone https://github.com/google/flatbuffers.git$ cd flatbuffers$ cmake -G &quot;Unix Makefiles&quot; -DCMAKE_BUILD_TYPE=Release -S . -B ./build$ cd build$ make -j$ make output &amp;&amp; make install DESTDIR=./output Check the contents in output/, the file hierarchy of installed package is as follows: $ tree.└── usr └── local ├── bin │ └── flatc # ---&gt; Flatbuffer compiler for code generation. ├── include │ └── flatbuffers │ ├── allocator.h │ ├── array.h │ ├── base.h │ ├── bfbs_generator.h │ ├── buffer.h │ ├── buffer_ref.h │ ├── code_generators.h │ ├── default_allocator.h │ ├── detached_buffer.h │ ├── flatbuffer_builder.h │ ├── flatbuffers.h # ---&gt; main header needs to include │ ├── flatc.h... │ └── verifier.h └── lib ├── cmake │ └── flatbuffers # ---&gt; cmake config for importing flatbuffer │ ├── BuildFlatBuffers.cmake │ ├── FlatBuffersTargets-release.cmake │ ├── FlatBuffersTargets.cmake │ ├── FlatcTargets-release.cmake │ ├── FlatcTargets.cmake │ ├── flatbuffers-config-version.cmake │ └── flatbuffers-config.cmake ├── libflatbuffers.a # ---&gt; static library needs to link └── pkgconfig └── flatbuffers.pc10 directories, 41 files Define The Schema And Generate the C++ headerWrite a demo schema: monster.fbs with the reference of the offical tutorial. namespace MyGame.Sample;enum Color:byte &#123; Red = 0, Green, Blue = 2 &#125;union Equipment &#123; Weapon &#125; // Optionally add more tablesstruct Vec3 &#123;\tx: float; // Here default value is unspecified, given it as 0\ty: float;\tz: float;&#125;table Monster &#123;\tpos: Vec3; // Struct\tmana: short = 150;\thp: short = 100;\tname: string;\tfriendly: bool = false (deprecated); // Will not genenate access function for deprecated field\tinventory: [ubyte]; // Vector of scalars\tcolor: Color = Blue; // Enum\tweapons: [Weapon]; // Vector of tables\tequipped:Equipment; // Union\tpath: [Vec3]; // Vector of struct&#125;table Weapon &#123;\tname: string;\tdamage: short;&#125;// What will be the root table for the serialized dataroot_type Monster; Q: Difference between struct and table? A: struct is better for data structure that will no change, since they use less memory and have faster lookup. Use flatc: $ ./flatc --cpp monster.fbs Some common options: --cpp: Generated a C++ header. -o PATH: Output all generated files to PATH(absolute or relative), if omited, PATH will be the current directory. Use FlatBuffers in Your Own CMake Project(1). Import FlatBuffers as a third-party library. include(FetchContent)FetchContent_Declare( flatbuffers GIT_REPOSITORY https://github.com/google/flatbuffers.git GIT_TAG acf39ff056df8c9e5bfa32cf6f7b5e6b87a90544 # version 22.12.06)FetchContent_MakeAvailable(flatbuffers) (2). Use flatc to generate C++ header with schema, and link flatbuffers in demo target. set(MONSTER_SCHEMA &quot;$&#123;CMAKE_CURRENT_LIST_DIR&#125;/monster.fbs&quot;)set(MONSTER_GENERATED_HEADER monster_generated.h)set(MONSTER_GENERATED_DIR $&#123;CMAKE_CURRENT_BINARY_DIR&#125;)add_custom_command(OUTPUT $&#123;MONSTER_GENERATED_HEADER&#125; COMMAND flatc --cpp -o $&#123;MONSTER_GENERATED_DIR&#125; $&#123;MONSTER_SCHEMA&#125; DEPENDS $&#123;MONSTER_SCHEMA&#125; flatc)add_custom_target(gen_monster_fbs DEPENDS $&#123;MONSTER_GENERATED_HEADER&#125;)add_executable(flatbuffers_demo flatbuffers_demo.cpp)target_link_libraries(flatbuffers_demo PRIVATE flatbuffers)target_include_directories(flatbuffers_demo PRIVATE $&#123;MONSTER_GENERATED_DIR&#125;)add_dependencies(flatbuffers_demo gen_monster_fbs) (3). Write demo C++ code to implement monster serialization and deserialization. (a). Monster Serialization void SerializeMonster(uint8_t *&amp;buf, int &amp;size) &#123; flatbuffers::FlatBufferBuilder builder(1024); /* Use basic builder to create all fields in monster */ auto weapon_one_name = builder.CreateString(&quot;Sword&quot;); short weapon_one_damage = 3; auto weapon_two_name = builder.CreateString(&quot;Axe&quot;); short weapon_two_damage = 5; auto sword = CreateWeapon(builder, weapon_one_name, weapon_one_damage); auto axe = CreateWeapon(builder, weapon_two_name, weapon_two_damage); auto name = builder.CreateString(&quot;Orc&quot;); unsigned char treasure[] = &#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;; auto inventory = builder.CreateVector(treasure, 10); std::vector&lt;flatbuffers::Offset&lt;Weapon&gt;&gt; weapon_vector; weapon_vector.push_back(sword); weapon_vector.push_back(axe); auto weapons = builder.CreateVector(weapon_vector); Vec3 points[] = &#123;Vec3(1.0f, 2.0f, 3.0f), Vec3(4.0f, 5.0f, 6.0f)&#125;; auto path = builder.CreateVectorOfStructs(points, 2); auto position = Vec3(1.0f, 2.0f, 3.0f); int hp = 300; int mana = 150; /* Create Monster */ // Usage 1：set all fileds at once// auto orc = CreateMonster(builder, &amp;position, mana, hp, name,// inventory, Color_Red, weapons,// Equipment_Weapon, axe.Union(), path); // Usage 2: set specified filed as needed MonsterBuilder monster_builder(builder); monster_builder.add_pos(&amp;position); monster_builder.add_hp(hp); monster_builder.add_name(name); monster_builder.add_mana(mana); monster_builder.add_inventory(inventory); monster_builder.add_color(Color_Red); monster_builder.add_weapons(weapons); monster_builder.add_equipped_type(Equipment_Weapon); monster_builder.add_equipped(axe.Union()); monster_builder.add_path(path); auto orc = monster_builder.Finish(); /* Invoke Finish() to end the serializing procedure. */ builder.Finish(orc); /* Get the serializing result: a byte buffer. */ buf = builder.GetBufferPointer(); size = builder.GetSize();&#125; (b). Monster Deserialization void DeserializeMonster(const uint8_t *buf) &#123;\t/* Get deserialized root object: monster *//\tauto monster = GetMonster(buf); /* Get each field of monster */ auto pos = monster-&gt;pos(); auto hp = monster-&gt;hp(); auto name = monster-&gt;name(); auto mana = monster-&gt;mana(); auto inventory = monster-&gt;inventory(); auto color = monster-&gt;color(); auto weapons = monster-&gt;weapons(); auto equipment_type = monster-&gt;equipped_type(); auto path = monster-&gt;path(); /* Access each field and print them */ cout &lt;&lt; &quot;pos: &#123; x: &quot; &lt;&lt; pos-&gt;x() &lt;&lt; &quot;, y: &quot; &lt;&lt; pos-&gt;y() &lt;&lt; &quot;, z: &quot; &lt;&lt; pos-&gt;z() &lt;&lt; &quot; &#125;&quot; &lt;&lt; endl; cout &lt;&lt; &quot;hp: &quot; &lt;&lt; hp &lt;&lt; endl; cout &lt;&lt; &quot;name: &quot; &lt;&lt; name-&gt;str() &lt;&lt; endl; cout &lt;&lt; &quot;mana: &quot; &lt;&lt; mana &lt;&lt; endl; cout &lt;&lt; &quot;inventory: &#123; &quot;; for (decltype(inventory-&gt;size()) i = 0; i &lt; inventory-&gt;size(); i++) &#123; cout &lt;&lt; static_cast&lt;uint32_t&gt;((*inventory)[i]); cout &lt;&lt; (i &lt; inventory-&gt;size() - 1 ? &quot;, &quot; : &quot; &quot;); &#125; cout &lt;&lt; &quot;&#125;&quot; &lt;&lt; endl; cout &lt;&lt; &quot;color: &quot; &lt;&lt; EnumNameColor(color) &lt;&lt; endl; cout &lt;&lt; &quot;weapons: &#123; &quot;; for (decltype(weapons-&gt;size()) i = 0; i &lt; weapons-&gt;size(); i++) &#123; const auto &amp;weapon = (*weapons)[i]; cout &lt;&lt; &quot;&#123;name: &quot; &lt;&lt; weapon-&gt;name()-&gt;str() &lt;&lt; &quot;, damage: &quot; &lt;&lt; weapon-&gt;damage() &lt;&lt; &quot;&#125;&quot;; cout &lt;&lt; (i &lt; weapons-&gt;size() - 1 ? &quot;, &quot; : &quot; &quot;); &#125; cout &lt;&lt; &quot;&#125;&quot; &lt;&lt; endl; cout &lt;&lt; &quot;equipment_type: &quot; &lt;&lt; EnumNameEquipment(equipment_type) &lt;&lt; endl; if (equipment_type == MyGame::Sample::Equipment_Weapon) &#123; auto equipped_weapon = monster-&gt;equipped_as_Weapon(); cout &lt;&lt; &quot;equipped: weapon &#123;name: &quot; &lt;&lt; equipped_weapon-&gt;name()-&gt;str() &lt;&lt; &quot;, damage: &quot; &lt;&lt; equipped_weapon-&gt;damage() &lt;&lt; &quot;&#125;&quot;; &#125; cout &lt;&lt; &quot;&#125;&quot; &lt;&lt; endl; cout &lt;&lt; &quot;path: &#123;&quot;; for (decltype(path-&gt;size()) i = 0; i &lt; path-&gt;size(); i++) &#123; const auto &amp;path_elem = (*path)[i]; cout &lt;&lt; &quot;&#123; pos: x: &quot; &lt;&lt; path_elem-&gt;x() &lt;&lt; &quot;, y: &quot; &lt;&lt; path_elem-&gt;y() &lt;&lt; &quot;, z: &quot; &lt;&lt; path_elem-&gt;z() &lt;&lt; &quot; &#125;&quot;; cout &lt;&lt; (i &lt; path-&gt;size() - 1 ? &quot;, &quot; : &quot; &quot;); &#125; cout &lt;&lt; &quot;&#125;&quot; &lt;&lt; endl;&#125; Demo output: Serialize Monster, buf: 0x7f77ae008b3c, size: 196Deserialize Monster:pos: &#123; x: 1, y: 2, z: 3 &#125;hp: 300name: Orcmana: 150inventory: &#123; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 &#125;color: Redweapons: &#123; &#123;name: Sword, damage: 3&#125;, &#123;name: Axe, damage: 5&#125; &#125;equipment_type: Weaponequipped: weapon &#123;name: Axe, damage: 5&#125;&#125;path: &#123;&#123; pos: x: 1, y: 2, z: 3 &#125;, &#123; pos: x: 4, y: 5, z: 6 &#125; &#125; The complete code is available at github VincentZhu007/flatbuffers_demo. More Reference How to Write A Schema","tags":["C++","FlatBuffer"]},{"title":"软件设计：一种C++项目的错误码设计实践","path":"/2022/12/11/software-deisgn-a cxx-error-code-design/","content":"2022.12.11 - V1.0 - 初稿 错误码作为应用软件的基础类，许多软件都实现了适合自身情况的错误码类，比如：leveldb的ErrCode类，TensorFlow Lite的TfLiteStatus 类。 为什么需要错误码类？ 错误码是软件可靠性的核心。 以C函数为例，对于一个C函数调用，返回的结果包含了两部分的信息，一是函数执行结果，二是函数执行成功与否，也就是执行状态。假若函数在执行过程中发生了难以继续的错误，就将对应的错误码返回，调用者也可以根据这个错误码，来调整执行流程，比如尝试重试或者记录日志等。 从软件设计的角度看，函数返回错误码和函数执行结果分别表示了控制面和数据面在某一个调用点的状态。 图示：函数调用过程的控制面和数据面 错误码的常见实现 1、单层错误码 enum ErrorCode &#123;\tkOk = 0,\tkFail = 1,\tkAccessError = 2,\tkOpenFileError = 3,\t...&#125; 2、不同模块的分段错误码 C风格： /* module error code */#define int ERR_MODULE_SOCKET = 1 &lt;&lt; 8;#define int ERR_MODULE_CONFIG = 2 &lt;&lt; 8;/* detail error code of socket module */#define int ERR_SOCKET_CONNECT = ERR_MODULE_SOCKET + 1#define int ERR_SOCKET_LISTEN = ERR_MODULE_SOCKET + 2#define int ERR_SOCKET_BIND = ERR_MODULE_SOCKET + 3#define int ERR_SOCKET_READ = ERR_MODULE_SOCKET + 4#define int ERR_SOCKET_WRITE = ERR_MODULE_SOCKET + 5/* detail error code of configuration file module */#define int ERR_CONFIG_READ = ERR_MODULE_CONFIG + 1#define int ERR_CONFIG_PARSE = ERR_MODULE_CONFIG + 2 C++风格： enum ErrorCode : int &#123;\tkSuccess = 0,\tkFailed = 1,\tkSocketErrorBase = 1000,\tkSocketConnectError = 1001,\tkSocketListenError = 1002,\tkSocketBindError = 1003,\tkSocketReadError = 1004,\tkSocketWriteError = 1005,\tkConfigErrorBase = 2000,\tkConfigReadError = 2001,\tkConfigParseError = 2002,&#125; 单纯错误码存在的问题： 1、越细节的错误越灵活，如果每一个具体错误，都需要加一种新的错误码，那不符合开闭原则，ErrorCode公共类收上层业务逻辑影响，导致变化。 2、对于细节的错误，单纯靠错误码难以理解，不利于快速找到错误位置。比如：对于kSocketReadError 这个错误码表示读取socket失败，但出现这个错误码的代码位置有多处，单靠这个错误码无法确定是哪一个执行路径导致的错误。 异常调用栈 异常时打印调用栈是一种清晰易定位的提示错误方式，但古老的语言，比如C自身不支持打印调用栈，除非借助于libwind等三方库，而且release版本一般没有符号表，因此打印的栈信息不完整。 另外，C++的异常机制比较复杂，影响性能的同时，也不能打印调用栈。 错误码+详细错误字符串 另一种思路是：对于大的错误类别，采用错误码，对于具体的错误上下文，由被调用的函数填充具体的错误消息字符串，比如打开某个具体的文件时，权限错误等等。 这种实践上有两个好处： 1、最终出现在日志中的，是带有上下文的错误信息，方便定位，并且决定打印的消息字符串日志级别的是上层调用者，而不是被调用的函数。举个简单的例子，在创建目录时如果发现目录存在就跳过，有一种实现方式是：直接尝试创建目录，如果调用mkdir创建失败，就打印：LOG_ERROR &lt;&lt; &quot;mkdir &quot; &lt;&lt; dir &lt;&lt; &quot; failed&quot;;，然后返回kDirExistedError错误码，但是对于上层调用者来说，这不是一个错误，而是一个符合预期的case，这里的LOG_ERROR是不必要的，从这里可以看出，有些场景下，执行失败对于应用来说是否是错误，应该有更高层的调用者来判断，也只有他具有支撑决策的上下文。 2、保留了错误码，可以让调用者高效地判断是否是可接受的异常情况。 总结成一句话：错误码给程序执行用，错误消息给调测人员看。 实现 enum class ErrorCode : int &#123; kOk = 0, kError = 1, kNullPoint = 2, kBadFuncParm = 3, kNoPermission = 4,&#125;;class Error &#123; public: Error() &#123;&#125;; Error(ErrorCode code) : error_code_(code) &#123;&#125; Error(ErrorCode code, const std::string &amp;message): error_code_(code) &#123; message_ = std::make_shared&lt;std::string&gt;(message); &#125; bool &amp;operator==(const Error &amp;) = delete; bool IsOk() &#123; return error_code_ == ErrorCode::kOk; &#125; bool IsError() &#123; return error_code_ == ErrorCode::kError; &#125; bool IsNullPoint() &#123; return error_code_ == ErrorCode::kNullPoint; &#125; bool EqualCode(ErrorCode code) &#123; return error_code_ == code; &#125; std::string Message() &#123; return (message_ != nullptr) ? (*message_) : &quot;&quot;; &#125; private: ErrorCode error_code_ = ErrorCode::kOk; std::shared_ptr&lt;std::string&gt; message_ = nullptr;&#125;;"},{"title":"开发工具-003-同一台机器使用多个gitee账号开发","path":"/2022/11/07/同一台机器使用多个gitee账号开发/","content":"背景：在个人开发的计算机上，一般只使用一个默认的ras秘钥对，但Gitee（GitHub也是如此，此处以Gitee为例）不允许多个gitee账号设置同一ssh公钥。 目标：在同一台机器上开发多个gitee账号下的项目。 解决方法：为不同的git项目指定不同的rsa秘钥对。 操作步骤： 1、生成一个新的自定义ssh密钥对。使用-C设置自定义的秘钥描述，并且，在交互弹出的文件保存路径时，要使用自定义的路径。如果直接回车会覆盖已有的id_rsa秘钥！！！ $ ssh-keygen -t rsa -C &#x27;YourCustomizedName&#x27;Generating public/private rsa key pair.Enter file in which to save the key (/Users/zgd/.ssh/id_rsa): /Users/zgd/.ssh/customized_rsa --这里可改成自定义的秘钥名称,与默认的id_rsa区分开来Enter passphrase (empty for no passphrase):Enter same passphrase again:...... 2、打开浏览器，使用第2个账号登入Gitee，添加这个新的ssh公钥。（具体可参加Gitee教程） 3、将第2个Gitee账号下的项目clone到本地，例如： $ git clone git@gitee.com:fatmouse007fatmouse007/clbench.git 此处改为你自己的工程路径，注意：必须使用git开头的仓库地址，这表示使用git协议，而不是https，只有使用git协议才会走ssh秘钥认证流程。 4、修改~/.ssh/config配置，为不同的url指定不同的秘钥，这一步是关键。 示例如下： Host gitee.com HostName gitee.com User git IdentityFile /Users/zgd/.ssh/163.rsa IdentitiesOnly yesHost fatmouse007.gitee.com HostName gitee.com User git IdentityFile /Users/zgd/.ssh/fatmouse007_rsa IdentitiesOnly yes 每一个配置项包含Host名称，以及实际的HostName，以及为这个url指定的私钥文件IdentityFile。以第2个配置项为例，当输入给ssh的url是fatmouse007.gitee.com，且用户名是git时，ssh会查找config，将url中 fatmouse007.gitee.com 部分替换为HostName gitee.com，并选择fatmouse007_rsa作为登录秘钥。如此，就实现了一个从自定义url到{实际url，自定义秘钥}的映射。 5、设置git工程的remote url中的真实HostName为自定义的Host $ git remote -vorigin\tgit@gitee.com:fatmouse007fatmouse007/clbench.git (fetch)origin\tgit@gitee.com:fatmouse007fatmouse007/clbench.git (push)$ git remote set-url origin git@fatmouse007.gitee.com:fatmouse007fatmouse007/clbench.git 这里将gitee.com替换成了fatmouse007.gitee.com，只是一个示例，具体请按照你实际的url替换。 6、验证配置是否成功 $ ssh -T git@fatmouse007.gitee.comHi fatmouse007! You&#x27;ve successfully authenticated, but GITEE.COM does not provide shell access. 以上表示ssh秘钥配置成功，以后可以直接git push。","tags":["git","ssh"],"categories":["开发工具"]},{"title":"软件开发成长路线图","path":"/2022/10/23/roadmap/","content":"果冻的编程路线图","tags":["路线图"],"categories":["个人规划"]},{"title":"动手学深度学习 - 02 训练一个线性回归模型","path":"/2022/10/07/动手学深度学习 - 02 训练一个线性回归模型/","content":"定义一个线性模型： y = Wx + B + \\epsilon其中，$x$ 和 $y$ 分别是输入输出数据，$W$ 和 $B$ 是参数，$\\epsilon$ 是随机噪声。 现在，我们使用mxnet的随机模块，生成特征$x$和标签$y$，使用反向传播来训练这个线性回归网络，而$W$和$B$就是网络中的权重。 from mxnet import autograd, nd 生成数据num_batches = 1000num_inputs = 3true_w = nd.array([[4.2, 2.0, -3.2]]).Ttrue_b = nd.array([[2.0]])x = nd.random.normal(0, 1, shape=(num_batches, num_inputs))epsilon = nd.random.normal(0, 0.01, shape=(num_batches, 1)) # 噪声服从均值为0,方差为0.1的正态分布y0 = nd.dot(x, true_w) + true_b + epsilony = y0 + epsilonprint(true_w.shape)print(true_b.shape)print(x.shape)print(y.shape)print(epsilon.shape)print(x[0:10])print(y[0:10]) (3, 1) (1, 1) (1000, 3) (1000, 1) (1000, 1) [[ 1.1630787 0.4838046 0.29956347] [ 0.15302546 -1.1688148 1.5580711 ] [-0.5459446 -2.3556297 0.5414402 ] [ 2.6785066 1.2546344 -0.54877394] [-0.68106437 -0.13531584 0.37723127] [ 0.41016445 0.5712682 -2.7579627 ] [ 1.07628 -0.6141326 1.8307649 ] [-1.1468065 0.05383795 -2.5074806 ] [-0.59164983 0.8586049 -0.22794184] [ 0.20131476 0.35005474 0.5360521 ]] &lt;NDArray 10x3 @cpu(0)&gt; [[ 6.9298854] [-4.676832 ] [-6.7715883] [17.516018 ] [-2.3353257] [13.697228 ] [-0.5571796] [ 5.297142 ] [ 1.9715714] [ 1.8121778]] &lt;NDArray 10x1 @cpu(0)&gt; 读取数据编写一个自定义的迭代器，来一批一批的获取数据。 import randomiter_batch_size = 10def data_iter(): # 产生一个随机索引序列 idx = list(range(num_batches)) # 生成索引序列：0,1,2,...,999 random.shuffle(idx) # 随机打乱索引 # 每次随机取10个数据 for i in range(0, num_batches, iter_batch_size): # 在不打乱的情况下，会生成：0，10, 20, 30 ..., 990 j = nd.array(idx[i : min(i + iter_batch_size, num_batches)]) # 这里设置取值范围：[0:10], [10:20],... yield nd.take(x, j), nd.take(y, j) # 迭代器获取数据 # 测试迭代器，拿2次n = 0for data, label in data_iter(): print(data, label) n += 1 if (n &gt;= 2): break [[ 4.2505121e+00 -2.1137807e-03 -7.9776019e-01] [ 4.9009416e-01 -1.1063820e+00 3.5573665e-02] [-1.2949859e-01 -3.0296946e-02 -1.7266597e-01] [-1.5107599e+00 -9.6534061e-01 5.4608238e-01] [ 7.8113359e-01 -1.1420447e+00 -2.8238511e-01] [ 2.6581082e-01 5.7875359e-01 -9.6763712e-01] [ 5.7725173e-01 3.8847062e-01 -1.2530572e+00] [ 1.1652315e+00 1.6189508e-01 -1.6221091e-01] [-2.1707486e-01 -6.4814579e-01 9.0141118e-01] [ 1.1282748e+00 2.5994456e+00 -2.0640564e+00]] &lt;NDArray 10x3 @cpu(0)&gt; [[22.389832 ] [ 1.707745 ] [ 1.9283633] [-8.028843 ] [ 3.9277847] [ 7.373327 ] [ 9.212286 ] [ 7.764362 ] [-3.1022215] [18.526558 ]] &lt;NDArray 10x1 @cpu(0)&gt; [[ 0.1787789 -0.18420085 -0.08212578] [-1.3121095 -0.04268014 -1.0699745 ] [ 0.20131476 0.35005474 0.5360521 ] [-1.365017 0.69103366 -0.4321104 ] [-0.04626409 -1.0672387 -2.0273046 ] [-0.6345202 -0.10353587 -1.3175181 ] [ 0.7618796 -1.1695448 0.7909283 ] [ 0.19799927 -0.10506796 -1.3348366 ] [ 1.6388271 0.59673244 1.1476266 ] [ 2.1935298 -0.5385921 -0.8611334 ]] &lt;NDArray 10x3 @cpu(0)&gt; [[ 2.6571708 ] [-0.15536022] [ 1.8121778 ] [-0.964692 ] [ 6.1714783 ] [ 3.366683 ] [ 0.34661472] [ 6.8972564 ] [ 6.4096384 ] [12.895386 ]] &lt;NDArray 10x1 @cpu(0)&gt; 初始化模型参数w = nd.random.normal(shape=(num_inputs, 1))b = nd.zeros((1,))params = [w, b]print(w)print(b)print(params) [[-1.3058136] [ 0.9344402] [ 0.5380863]] &lt;NDArray 3x1 @cpu(0)&gt; [0.] &lt;NDArray 1 @cpu(0)&gt; [ [[-1.3058136] [ 0.9344402] [ 0.5380863]] &lt;NDArray 3x1 @cpu(0)&gt;, [0.] &lt;NDArray 1 @cpu(0)&gt;] 为参数创建梯度信息，为后面的参数求导做准备。 for param in params: param.attach_grad() 定义模型def net(x): return nd.dot(x, w) + b # 线性模型的预测值y_hat # 测试一下模型net(data) ​ [[-0.44976735] [ 1.0977497 ] [ 0.35266796] [ 2.1956747 ] [-2.0277233 ] [ 0.02287859] [-1.6621547 ] [-1.0749872 ] [-0.9648698 ] [-3.8309872 ]] 定义损失函数def square_loss(yhat, y): # 将y reshape，避免自动广播 return (yhat - y.reshape(yhat.shape)) ** 2 定义优化求解策略：随机梯度下降SGDdef sgd(params, learning_rate): for param in params: param[:] = param - learning_rate * param.grad 训练模型epochs = 5learning_rate = .001for e in range(epochs): total_loss = 0 for data, label in data_iter(): # 一次迭代学习过程 with autograd.record(): output = net(data) loss = square_loss(output, label) loss.backward() sgd(params, learning_rate) total_loss += nd.sum(loss).asscalar() print(&quot;Epoch %d, average loss: %f&quot; % (e, total_loss/num_batches)) Epoch 0, average loss: 12.393637 Epoch 1, average loss: 0.164320 Epoch 2, average loss: 0.002606 Epoch 3, average loss: 0.000418 Epoch 4, average loss: 0.000386 训练完，比较学到的参数和真实参数 true_b, b ( [[2.]] &lt;NDArray 1x1 @cpu(0)&gt;, [2.0011895] &lt;NDArray 1 @cpu(0)&gt;) true_w, w ( [[ 4.2] [ 2. ] [-3.2]] &lt;NDArray 3x1 @cpu(0)&gt;, [[ 4.1996527] [ 2.000402 ] [-3.199496 ]] &lt;NDArray 3x1 @cpu(0)&gt;) 可以看到训练得到的参数，与真实参数非常接近。并且，随着训练次数的增加，损失函数值逐步收敛。 参考B站李沐课程[MXNet/Gluon] 动手学深度学习第一课：从上手到多类分类","categories":["深度学习","动手学深度学习"]},{"title":"动手学深度学习 - 01 开发环境搭建","path":"/2022/10/06/动手学深度学习 - 01 开发环境搭建/","content":"这是跟随李沐大神《动手学深度学习》的课程笔记。 1、概念区分 人工智能：让机器具备像人一样的智能。 机器学习：不用显式给机器编程，让机器自身通过学习来获取问题的解法。 深度学习：使用神经网络等更复杂的模型来学习。 范围包含关系：人工智能 &gt; 机器学习 &gt; 深度学习 2、开发环境安装MacOS安装开发环境，参考：http://zh.gluon.ai/chapter_prerequisite/install.html 在已安装Anaconda的前提下，执行以下命令： mkdir d2l-zh &amp;&amp; cd d2l-zhcurl https://zh.d2l.ai/d2l-zh-1.1.zip -o d2l-zh.zipunzip d2l-zh.zip &amp;&amp; rm d2l-zh.zipconda env update -f environment.ymlconda activate gluonjupyter notebook 启动效果：","categories":["深度学习","动手学深度学习"]},{"title":"读LevelDB - 01 开篇&编译源码","path":"/2022/10/05/读LevelDB - 01 开篇&编译源码/","content":"LevelDB简介LevelDB是Google开源的一个键值数据库，它使用C++实现，提供”字符串类型键值对“的持久化能力。 为什么选择读它？在进入具体的”读“之前，先补充说明一个问题：为什么选择LevelDB来读？ 就我自己而言，原因有两点： 1、LevelDB是数据库的一种最简实现。LevelDB只提供了对键值对的持久化和事务操作，在众多开源数据库软件中，比如MySQL、SQLite等，LevelDB可以说算是一种最简数据库实现，通过解读它可以对最关键、最必要的数据库特性做一个了解，这样在后续数据库解读中也有助于抓住主要矛盾。 2、LevelDB是一个C++库。通过读它来丰富C++运用技巧。 从源码构建好的，现在进入正题，读代码。 读代码的第一件事，就是编译源码了。通过编译源码，可以快速了解代码的结构，搭建代码跟踪调试的环境，为下一步的流程跟踪打好基础。 LevelDB的代码仓地址是：https://github.com/google/leveldb。 从READM文档可以看到，LevelDB采用了git submodule来管理依赖的三方件，并使用CMake作为工程的构建工具。 接下来，开始在本地环境构建工程，我的开发环境是MacOS，采用Git+CMake+CLang构建： （1）、下载代码仓到本地（建议先fork到自己的个人仓，再从个人仓clone）； git clone --recurse-submodules https://github.com/google/leveldb.git 加--recurse-submodules选项，会自动clone三方库到third_party/子目录。 （2）、构建工程； mkdir -p build &amp;&amp; cd buildcmake -DCMAKE_BUILD_TYPE=Release .. &amp;&amp; cmake --build . （3）、执行用例，检查LevelDB是否正常。 ctest 执行结果如下： Test project /Users/zgd/Documents/SourceCode/leveldb/build Start 1: leveldb_tests1/3 Test #1: leveldb_tests .................... Passed 189.94 sec Start 2: c_test2/3 Test #2: c_test ........................... Passed 2.44 sec Start 3: env_posix_test3/3 Test #3: env_posix_test ................... Passed 0.28 sec100% tests passed, 0 tests failed out of 3Total Test time (real) = 192.66 sec 用例通过，源码构建完成。 好了，以上就是读LevelDB的开篇内容。","categories":["LevelDB源码剖析"]},{"title":"设计模式(6)-建造模式","path":"/2022/07/20/design-pattern-06-builder/","content":"建造模式是什么？建造模式是一种创建对象的方式，通过使用建造模式，可以将对象的内部状态表示和对象的创建过程分割开。 某些情况下对象内部的状态必须在一定的规则约束下才有意义，典型的几种情况如下： （1）对象内部的多个属性字段要同时有效。比如：创建一封电子邮件，需要填好发件人、收件人、主题、内容等，才能作为一封完整的邮件。而填写过程，可以用专门的建造方法来负责实现，其返回的结果是一个有效的邮件对象。 （2）对象需要按照一定的顺序创建。比如组装汽车，要先组装内部的器件：发动机、变速箱、车轮、座椅等，再组装外部车身。 简单来说，这些情况下对象的创建流程比较复杂，而且和对象内部的实现细节高度相关。因此，创建的责任应该交由对象的提供者，而不是使用者，因为使用者不知道对象的内部实现，而且也不应该知道。这时产品的提供者就可以使用建造模式，提供建造类和建造方法，供用户使用。 建造模式类图的一种形式如下： 从类图中可以看到，本来属于Product的内部性质零件Part1和Part2的建造被放到ConcreteBuilder中，这是一种“性质外部化”，将部分状态的设置放到外部类中，这样，Product的属性可以更加通用、单一，具体的业务需求对应的产品生产流程，作为一个频繁变化的点，交由builder处理，将变化点（构建流程）统一存放，这样能很好地控制需求变化代码的软件修改复杂度。 构建模式 vs 抽象工厂模式构建模式和抽象工厂模式非常相似，它们的区别在于： 抽象工厂模式中的工厂返回的是一个完整的对象，核心关注点是产品品类的不断增加。 构建模式中的构造类是一步一步完成对象的创建的，核心关注点是产品创建的流程/规则的变化。 参考《Java与模式》第19章","categories":["设计模式","创建型模式"]},{"title":"设计模式(5)-单例模式","path":"/2022/07/20/design-pattern-05-singleton/","content":"单例模式是什么？单例模式，顾名思义，就是整个类只提供一个实例对象，供外部使用。 从设计模式的角度看，单例模式是一种退化的简单工厂模式，其中工厂的创建方法就是产品类自身的静态方法，并且创建出的产品对象只有一个，循环使用。 单例模式的三个要素是：（1）单例类只有一个实例对象；（2）单例类必须自行创建此实例对象；（3）单例类必须自行向系统提供此实例对象。 为了得到单例，需要对类的构造函数进行限制，就C++而言，需要设置类构造函数为私有方法，并提供静态接口获取提供的单例。 C++单例实现在C++11的代码实现中，使用较多的是Meyers单例实现，简单好用，它的类图如下： 示例代码如下： #include &lt;iostream&gt;#include &lt;string&gt;class Singleton &#123;public: static Singleton &amp;GetInstance() &#123; static Singleton instance; return instance; &#125; void SetStr(const std::string &amp;str) &#123; str_ = str; &#125; std::string GetStr() const &#123; return str_; &#125;private: Singleton() = default; ~Singleton() = default; Singleton(const Singleton&amp; rhs) = delete; Singleton &amp;operator=(const Singleton&amp; rhs) = delete; std::string str_;&#125;;int main()&#123; auto &amp;singletonA = Singleton::GetInstance(); singletonA.SetStr(&quot;This is a singleton.&quot;); auto &amp;singletonB = Singleton::GetInstance(); std::cout &lt;&lt; singletonB.GetStr() &lt;&lt; std::endl;&#125; 输出： This is a singleton. Meyers单例的核心思路是利用静态变量的构造过程位于main函数之前这一特性，避免多个线程调用GetInstance()接口带来的并发构造实例问题，不使用锁仍可做到线程安全。 这是一种饿汉式实现，单例还有其它实现方式，比如double check实现等，这里不一一介绍。 参考《Java与模式》第15章","tags":["单例"],"categories":["设计模式","创建型模式"]},{"title":"设计模式(4)-抽象工厂模式","path":"/2022/07/20/design-pattern-04-abstract-factory/","content":"抽象工厂模式是什么？前面两节分别介绍了简单工厂模式和工厂模式。其中，简单工厂模式提供了单个创建接口，通过指定入参来创建不同的具体产品，比如水果农场例子中的苹果和葡萄。工厂模式则利用多态性，采用抽象工厂和具体的苹果工厂、葡萄工厂来实现创建水果对象的功能，生产新的水果只需添加新的具体工厂即可，符合“开闭原则”。 工厂模式中，一个抽象工厂可以处理一种抽象产品，那么，如果是两种抽象产品呢？如果这两种产品没有关联，自然可以使用两个抽象工厂来处理，仍旧使用工厂模式。 一种特殊的情况是：这两种产品的层次结构是类似的，一个典型的例子是常见的图形界面，无论在windows、linux、还是macos，都有窗口、按钮、文本框、复选框等，不同之处只是UI风格不同而已。通过提取多个抽象产品的层次结构，抽象出一个相应的工厂层次结构。 抽象工厂模式类图如下： 从图中右侧可以看出，这个模式下的产品存在两种划分：（1）横向上，同一抽象产品下 AbstracetProductA 的两种具体产品ConcreteProductA1 和 ConcreteProductA2，对应于左侧两种不同的具体工厂类 ConcreteFactory1 和 ConcreteFactory2，比如：windows的Push按钮和Toggle按钮。（2）纵向上，在不同产品体系中的两种类似的产品，ConcreteProductA1 和 ConcreteProductB1，对应于左侧同一具体工厂类的两种方法：CreateProductA()和CreateProductB()。比如：windows的Push按钮和macos的Push按钮。 如何确定哪些具体产品归纳到横向划分呢，其实简单归纳就是同一产品族，windows上的不同按钮都是为windows视窗系统这个大产品的其中一个组件。反过来，windows的Push按钮和macos的Push按钮不能组成一个具体产品，风格完全不搭。 最后，思考一下，为什么这个模式取名为抽象工厂模式，个人理解是它将工厂从只生产某一类产品的角色中抽象出来，得到能生产多种相同产品体系的一个抽象的工厂。一个生活中的例子：服装厂夏天可以生产男式T恤和女士T恤，冬天生产男式羽绒服和女士羽绒服，从中就可以抽出一个生产男式衣服和女式衣服的“抽象”服装厂，按季节生产不同的具体服装。 代码示例暂略 参考《Java与模式》第14章","tags":["工厂模式"],"categories":["设计模式","创建型模式"]},{"title":"设计模式(3)-工厂模式","path":"/2022/07/20/design-pattern-03-factory/","content":"工厂模式是什么？工厂模式又称为虚拟构造子模式或多态工厂模式，是在简单工厂模式的基础上，将核心角色“工厂类”改为层次继承结构，提取出一个抽象工厂的角色，由对应具体产品的具体工厂类来继承此抽象工厂，并提供抽象工厂的多态性，对外提供统一的产品创建接口。 工厂模式示意图如下所示。可以看出，创建产品的职责不再和简单工厂模式那样，集中在单个工厂类，而是分散到各具体工厂类，这样，既继承了简单工厂模式的优点：由工厂类承担创建产品的责任，客户类只需“消费”产品；又避免了其单个工厂类责任过于集中，新增产品需修改工厂类源代码的缺点。 工厂模式示例：水果农场 /** * 工厂模式 */#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;algorithm&gt;class Fruit &#123;public: virtual void grow() = 0; virtual void plant() = 0; virtual void harvest() = 0;&#125;;class Apple : public Fruit &#123;public: void grow() override &#123; std::cout &lt;&lt; &quot;grow apple&quot; &lt;&lt; std::endl; &#125; void plant() override &#123; std::cout &lt;&lt; &quot;plant apple&quot; &lt;&lt; std::endl; &#125; void harvest() override &#123; std::cout &lt;&lt; &quot;harvest apple&quot; &lt;&lt; std::endl; &#125; void SetTreeAge(uint32_t treeAge) &#123; treeAge_ = treeAge; &#125; uint32_t GetTreeAge() const &#123; return treeAge_; &#125;private: uint32_t treeAge_ = 0;&#125;;class Grape : public Fruit &#123;public: void grow() override &#123; std::cout &lt;&lt; &quot;grow grape&quot; &lt;&lt; std::endl; &#125; void plant() override &#123; std::cout &lt;&lt; &quot;plant grape&quot; &lt;&lt; std::endl; &#125; void harvest() override &#123; std::cout &lt;&lt; &quot;harvest grape&quot; &lt;&lt; std::endl; &#125; void SetSeedless(bool seedless) &#123; seedless_ = seedless; &#125; bool IsSeedledd() const &#123; return seedless_; &#125;private: bool seedless_ = false;&#125;;class FruitGardener &#123;public: virtual Fruit *Factory() = 0;&#125;;class AppleGardener: public FruitGardener &#123;public: Fruit * Factory() override &#123; return new Apple(); &#125;&#125;;class GrapeGardener: public FruitGardener &#123;public: Fruit * Factory() override &#123; return new Grape(); &#125;&#125;;int main()&#123; std::cout &lt;&lt; &quot;=== create apple ===&quot; &lt;&lt; std::endl; FruitGardener *appleGardener = new AppleGardener(); auto *apple = appleGardener-&gt;Factory(); apple-&gt;grow(); apple-&gt;harvest(); apple-&gt;plant(); std::cout &lt;&lt; &quot;=== create grape ===&quot; &lt;&lt; std::endl; FruitGardener *grapeGardener = new GrapeGardener(); auto *grape = grapeGardener-&gt;Factory(); grape-&gt;grow(); grape-&gt;harvest(); grape-&gt;plant(); delete apple; delete grape; delete appleGardener; delete grapeGardener;&#125; 参考《Java与模式》第13章","tags":["工厂模式"],"categories":["设计模式","创建型模式"]},{"title":"设计模式(2)-简单工厂模式","path":"/2022/07/20/design-pattern-02-simple-factory/","content":"简单工厂模式是什么？简单工厂是指在基类中对一系列具有相同行为的对象按需创建的方法。被创建的对象通常具有共同的基类。 类图如下： 简单工厂示例：水果 /** * 简单工厂模式 */#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;algorithm&gt;class Fruit &#123;public: virtual void grow() = 0; virtual void plant() = 0; virtual void harvest() = 0;&#125;;class Apple : public Fruit &#123;public: void grow() override &#123; std::cout &lt;&lt; &quot;grow apple&quot; &lt;&lt; std::endl; &#125; void plant() override &#123; std::cout &lt;&lt; &quot;plant apple&quot; &lt;&lt; std::endl; &#125; void harvest() override &#123; std::cout &lt;&lt; &quot;harvest apple&quot; &lt;&lt; std::endl; &#125; void SetTreeAge(uint32_t treeAge) &#123; treeAge_ = treeAge; &#125; uint32_t GetTreeAge() const &#123; return treeAge_; &#125;private: uint32_t treeAge_ = 0;&#125;;class Grape : public Fruit &#123;public: void grow() override &#123; std::cout &lt;&lt; &quot;grow grape&quot; &lt;&lt; std::endl; &#125; void plant() override &#123; std::cout &lt;&lt; &quot;plant grape&quot; &lt;&lt; std::endl; &#125; void harvest() override &#123; std::cout &lt;&lt; &quot;harvest grape&quot; &lt;&lt; std::endl; &#125; void SetSeedless(bool seedless) &#123; seedless_ = seedless; &#125; bool IsSeedledd() const &#123; return seedless_; &#125;private: bool seedless_ = false;&#125;;class FruitGardener &#123;public: static Fruit *Factory(const std::string &amp;fruit) &#123; if (fruit == &quot;apple&quot;) &#123; return new Apple(); &#125; else if (fruit == &quot;grape&quot;) &#123; return new Grape(); &#125; else &#123; std::cerr &lt;&lt; &quot;bad fruit to create&quot; &lt;&lt; std::endl; return nullptr; &#125; &#125;&#125;;int main()&#123; std::cout &lt;&lt; &quot;=== create apple ===&quot; &lt;&lt; std::endl; auto *apple = FruitGardener::Factory(&quot;apple&quot;); apple-&gt;grow(); apple-&gt;harvest(); apple-&gt;plant(); std::cout &lt;&lt; &quot;=== create grape ===&quot; &lt;&lt; std::endl; auto *grape = FruitGardener::Factory(&quot;grape&quot;); grape-&gt;grow(); grape-&gt;harvest(); grape-&gt;plant(); delete apple; delete grape;&#125; 输出： === create apple ===grow appleharvest appleplant apple=== create grape ===grow grapeharvest grapeplant grape 参考《Java与模式》第12章","tags":["工厂模式"],"categories":["设计模式","创建型模式"]},{"title":"设计模式(1)-开篇","path":"/2022/07/20/design-pattern-01-startup/","content":"Hi，这是设计模式专栏的开篇！","tags":["设计模式"],"categories":["设计模式","创建型模式"]},{"title":"快速开始","path":"/wiki/jetnn/p1-startup.html","content":"你好，本文介绍JetNN快速开始。"},{"title":"关于我","path":"/about/index.html","content":"您好！ 我是一名C++软件工程师，目前的工作内容是深度学习推理框架的开发，日常与Bug和性能battle中 感兴趣的方向有：Linux、设计模式、微型飞行器。"},{"title":"快速开始","path":"/wiki/jetconf/p1-startup.html","content":"你好，本文介绍快速开始。"},{"title":"JetNN设计","path":"/wiki/jetnn/p0-design.html","content":"你好，本文介绍JetNN的设计。"},{"title":"JetConf设计","path":"/wiki/jetconf/p0-design.html","content":"你好，本文介绍JetConf的设计。"}]